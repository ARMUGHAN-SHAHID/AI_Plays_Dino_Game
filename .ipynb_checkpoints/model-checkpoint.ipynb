{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armughan/anaconda3/envs/py3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/armughan/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():#used to store parameter values\n",
    "    def __init__(self,params):\n",
    "        #STORING PARAMETER VALUES \n",
    "        self.input_shape=params['input_shape']\n",
    "        self.num_outputs=params['num_outputs']\n",
    "        self.layer_hierarchy=params['layer_hierarchy']\n",
    "        self.activation_fn=params.get('activation_fn',tf.nn.relu)\n",
    "        self.loss_fn=params.get('loss_fn',tf.losses.softmax_cross_entropy)\n",
    "        self.learning_rate=params['learning_rate']\n",
    "        self.optimizer_fn=params['optimizer_fn']\n",
    "        self.initializer_fn=params['initializer_fn']\n",
    "        self.name_scope=params['name_scope']\n",
    "        logdir=params['logdir']\n",
    "#         self.step_no=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model():\n",
    "    def __init__(self,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        #STORING PARAMETER VALUES\n",
    "        if not restore_params:\n",
    "            self.params=Params(param_dict)\n",
    "        else:\n",
    "            self.restore_params_fn(pickle_file_path)\n",
    "\n",
    "    def form_placeholder(self,shape,dt=tf.float32):\n",
    "        X=tf.placeholder(dt,shape=shape)\n",
    "        return X\n",
    "    def from_variable(self,shape,dt=tf.float32,name=\"\",trainable=True,initail_val):\n",
    "        if name==\"\":\n",
    "            return tf.Variable(initial_value=initail_val,trainable=trainable,dtype=dt)\n",
    "        else:\n",
    "            initializer=tf.constant_initializer(initail_val)\n",
    "            return tf.get_variable(name=name,shape=shape, dtype=dt,initializer=initializer,trainable=trainable)\n",
    "    \n",
    "    def form_convolutional_layer(self,inputs,layer_params):\n",
    "        return tf.layers.conv2d(\n",
    "                    inputs=inputs,\n",
    "                    filters=layer_params['num_filters'],\n",
    "                    kernel_size=layer_params['kernel_size'],\n",
    "                    strides=layer_params['kernel_strides'],\n",
    "                    padding=layer_params['padding'],\n",
    "                    kernel_initializer=self.params.initializer_fn(),\n",
    "                    activation=None)\n",
    "    def form_max_pooling_layer(self,inputs,layer_params):\n",
    "        tf.layers.max_pooling2d(\n",
    "                    inputs=inputs,\n",
    "                    pool_size=layer_params['pool_size'],\n",
    "                    strides=layer_params['pool_strides'])\n",
    "    def form_activation_layer(self,inputs):\n",
    "        return self.params.activation_fn(inputs)\n",
    "    \n",
    "    def form_fc_layer(self,inputs,layer_params):\n",
    "        return tf.layers.dense(inputs,layer_params['num_hidden_units'],activation=None,kernel_initializer=self.params.initializer_fn())\n",
    "    \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(onehot_labels=targets,logits=logits)\n",
    "        return entropies\n",
    "    \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            self.X=self.form_placeholder(self.params.input_shape)\n",
    "            self.Y=self.form_placeholder((None,self.params.num_outputs),tf.float32)\n",
    "            self.lr_placeholder=self.form_placeholder([]) #since we can change learning arate during training\n",
    "            inputs=self.X\n",
    "            for layer_params in self.params.layer_hierarchy:\n",
    "                if layer_params['layer_type']=='conv_layer':\n",
    "                    inputs=self.form_convolutional_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='fc_layer':\n",
    "                    inputs=self.form_fc_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='activation_layer':\n",
    "                    inputs=self.form_activation_layer(inputs)\n",
    "                elif layer_params['layer_type']=='pooling_layer':\n",
    "                    inputs=self.form_max_pooling_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='flattening_layer':\n",
    "                    inputs=tf.contrib.layers.flatten(inputs)\n",
    "\n",
    "    #         making logits layer (final output layer)\n",
    "            self.logits=tf.layers.dense(inputs,self.params.num_outputs,activation=None,kernel_initializer=self.params.initializer_fn())\n",
    "\n",
    "            self.loss=self.form_loss(self.logits,self.Y)\n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.train_op=optimizer.minimize(loss = self.loss)\n",
    "            \n",
    "            self.initializer=tf.global_variables_initializer()\n",
    "        model_variables=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.params.name_scope)#saving only the varuiables belonging to this scope\n",
    "        self.saver=tf.train.Saver(var_list=model_variables)\n",
    "        \n",
    "    def save_model(self,sess,savedir=\"/\"):\n",
    "        step=self.params.step_no\n",
    "        savedir=os.path.join(os.getcwd(),savedir)\n",
    "        if not hasattr(self,'saved_before'):#calling save model for the first time\n",
    "            if not os.path.isdir(savedir):#creating directory if not exists\n",
    "                try:  \n",
    "                    os.makedirs(savedir)\n",
    "                except OSError:\n",
    "                    print ('failed to make the specified_directory.Returning...')\n",
    "                    return\n",
    "            file_pi = open(savedir+\"\\\\\"+self.params.name_scope+\".pkl\", 'wb+') #saving param object\n",
    "            pickle.dump(self.params, file_pi)\n",
    "            #saving tensorflow graph and weight values\n",
    "            self.saver.save(sess,savedir+\"\\\\\"+self.params.name_scope+\".ckpt\", global_step=step) #saving model weights\n",
    "            self.saved_before=True\n",
    "        else:    #saving model weights\n",
    "            self.saver.save(sess,savedir+\"\\\\\"+self.params.name_scope+\".ckpt\", global_step=step,write_meta_graph=False)#writes meta graph for the first time save_model is called\n",
    "    def restore_params_fn(self,pickle_file_path):\n",
    "        if os.path.exists(pickle_file_path):\n",
    "            filehandler = open(pickle_file_path, 'rb')\n",
    "            self.params=pickle.load(filehandler)\n",
    "        else:\n",
    "            print(\"no such file exists\")\n",
    "        \n",
    "    def restore_model(self,sess,restore_dir):\n",
    "        restore_dir=os.path.join(os.getcwd(),restore_dir)\n",
    "        print (restore_dir+\"\\\\\"+self.params.name_scope)\n",
    "        self.saver.restore(sess, tf.train.latest_checkpoint(restore_dir+\"\\\\\"))#loading latest model\n",
    "        \n",
    "    def train(self,sess,n_epochs,n_iter,get_next_batch_fn,save_every_n_iter,log_every_n_iter,save_dir,initialize=False):\n",
    "        if initialize:\n",
    "            sess.run([self.initializer])\n",
    "        for epoch in np.arange(n_epochs):\n",
    "            print(\"----Epoch=\"+str(epoch)+\"\\n\")\n",
    "            it=0\n",
    "            for x,y in get_next_batch_fn():\n",
    "                it+=1\n",
    "                if it>=n_iter:\n",
    "                    break\n",
    "                feed_dict={self.X:x,self.Y:y,self.lr_placeholder:self.params.learning_rate}\n",
    "                loss,_=sess.run([self.loss,self.train_op],feed_dict=feed_dict)\n",
    "                self.params.step_no+=1\n",
    "                if (self.params.step_no)%save_every_n_iter==0:\n",
    "                    print(\"saving model\\n\")\n",
    "                    self.save_model(sess,save_dir)\n",
    "#                 print (\"\"\"loss= \"+str(loss)+\"\\n\")\n",
    "                print (\"Step={} and loss occured= {} \\n\".format(str(self.params.step_no),str(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'input_shape':[None, 35, 190, 1],\n",
    "    'num_outputs':3,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':1,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':1,'num_filters':16,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':256},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':100},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.relu,\n",
    "    'loss_fn':tf.losses.softmax_cross_entropy,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'neural_network_v1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing model \n",
    "# tf.reset_default_graph()\n",
    "# model=CNN_Model(params)\n",
    "# model.build_model()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(model.initializer)\n",
    "#     model.save_model(sess,\"1st_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing restoring a new model\n",
    "# tf.reset_default_graph()\n",
    "# model=CNN_Model(restore_params=True,pickle_file_path=\"1st_try/neural_network_v1.pkl\")\n",
    "# model.build_model()\n",
    "# with tf.Session() as sess:\n",
    "#     model.restore_model(sess,\"1st_try\")\n",
    "# #     sess.run(model.initializer)\n",
    "# #     model.save_model(sess,\"1st_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size= 9964\n"
     ]
    }
   ],
   "source": [
    "batch_size=120\n",
    "data=Data('training_data.npy',batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dino game\\AI_Plays_Dino_Game\\1st_try\\neural_network_v1\n",
      "INFO:tensorflow:Restoring parameters from D:\\dino game\\AI_Plays_Dino_Game\\1st_try\\neural_network_v1.ckpt-15\n",
      "----Epoch=0\n",
      "\n",
      "Step=6 and loss occured= 1.0755359 \n",
      "\n",
      "Step=7 and loss occured= 1.0725709 \n",
      "\n",
      "Step=8 and loss occured= 1.0731826 \n",
      "\n",
      "Step=9 and loss occured= 1.0714436 \n",
      "\n",
      "saving model\n",
      "\n",
      "Step=10 and loss occured= 1.0672069 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-18ddedbba909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#     sess.run(model.initializer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#     model.save_model(sess,\"1st_try\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1st_try\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#     for x,y in data.get_next_batch():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#         print (x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-b19a54892c6c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, n_epochs, n_iter, get_next_batch_fn, save_every_n_iter, log_every_n_iter, save_dir, initialize)\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=50\n",
    "n_iter=int(data.X_train.shape[0]/data.batch_size)\n",
    "save_every_n_iter=5\n",
    "log_every_n_iter=20\n",
    "initialize=False\n",
    "tf.reset_default_graph()\n",
    "# model=CNN_Model(params)\n",
    "\n",
    "    \n",
    "\n",
    "model=\"\"\n",
    "with tf.Session() as sess:\n",
    "    if(not initialize):\n",
    "        model=CNN_Model(restore_params=True,pickle_file_path=\"1st_try/neural_network_v1.pkl\")\n",
    "        model.build_model()\n",
    "        model.restore_model(sess,\"1st_try\")\n",
    "        model.params.learning_rate=0.0003\n",
    "    else:\n",
    "        model=CNN_Model(params)\n",
    "        model.build_model()\n",
    "#         model.params.step_no=765\n",
    "#     sess.run(model.initializer)\n",
    "#     model.save_model(sess,\"1st_try\")\n",
    "    model.train(sess,n_epochs,n_iter,data.get_next_batch,save_every_n_iter,log_every_n_iter,save_dir=\"1st_try\",initialize=initialize)\n",
    "#     for x,y in data.get_next_batch():\n",
    "#         print (x.shape)\n",
    "#         print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
