{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armughan/anaconda3/envs/py3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/armughan/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():#used to store parameter values\n",
    "    def __init__(self,params):\n",
    "        #STORING PARAMETER VALUES \n",
    "        self.input_shape=params['input_shape']\n",
    "        self.num_outputs=params['num_outputs']\n",
    "        self.layer_hierarchy=params['layer_hierarchy']\n",
    "        self.activation_fn=params.get('activation_fn',tf.nn.relu)\n",
    "        self.loss_fn=params.get('loss_fn',tf.losses.softmax_cross_entropy)\n",
    "        self.learning_rate=params['learning_rate']\n",
    "        self.optimizer_fn=params['optimizer_fn']\n",
    "        self.initializer_fn=params['initializer_fn']\n",
    "        self.name_scope=params['name_scope']\n",
    "        logdir=params['logdir']\n",
    "#         self.step_no=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model():\n",
    "    def __init__(self,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        #STORING PARAMETER VALUES\n",
    "        if not restore_params:\n",
    "            self.params=Params(param_dict)\n",
    "        else:\n",
    "            self.restore_params_fn(pickle_file_path)\n",
    "\n",
    "    def form_placeholder(self,shape,dt=tf.float32):\n",
    "        X=tf.placeholder(dt,shape=shape)\n",
    "        return X\n",
    "    def form_variable(self,shape,dt=tf.float32,name=\"\",trainable=True,initializer=tf.zeros_initializer):\n",
    "        if name==\"\":\n",
    "            return tf.Variable(initial_value=initializer,trainable=trainable,dtype=dt)\n",
    "        else:\n",
    "#             initializer=tf.constant_initializer(initail_val)\n",
    "            return tf.get_variable(name=name,shape=shape, dtype=dt,initializer=initializer(),trainable=trainable)\n",
    "    \n",
    "    def form_convolutional_layer(self,inputs,layer_params):\n",
    "        return tf.layers.conv2d(\n",
    "                    inputs=inputs,\n",
    "                    filters=layer_params['num_filters'],\n",
    "                    kernel_size=layer_params['kernel_size'],\n",
    "                    strides=layer_params['kernel_strides'],\n",
    "                    padding=layer_params['padding'],\n",
    "                    kernel_initializer=self.params.initializer_fn(),\n",
    "                    activation=None)\n",
    "    def form_max_pooling_layer(self,inputs,layer_params):\n",
    "        tf.layers.max_pooling2d(\n",
    "                    inputs=inputs,\n",
    "                    pool_size=layer_params['pool_size'],\n",
    "                    strides=layer_params['pool_strides'])\n",
    "    def form_activation_layer(self,inputs):\n",
    "        return self.params.activation_fn(inputs)\n",
    "    \n",
    "    def form_fc_layer(self,inputs,layer_params):\n",
    "        return tf.layers.dense(inputs,layer_params['num_hidden_units'],activation=None,kernel_initializer=self.params.initializer_fn())\n",
    "    \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(onehot_labels=targets,logits=logits)\n",
    "        return entropies\n",
    "    \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            self.X=self.form_placeholder(self.params.input_shape)\n",
    "            self.Y=self.form_placeholder((None,self.params.num_outputs),tf.float32)\n",
    "            self.lr_placeholder=self.form_placeholder([]) #since we can change learning arate during training\n",
    "            self.step_no=self.form_variable(shape=[],name=\"step_no\",trainable=False)\n",
    "            inputs=self.X\n",
    "            for layer_params in self.params.layer_hierarchy:\n",
    "                if layer_params['layer_type']=='conv_layer':\n",
    "                    inputs=self.form_convolutional_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='fc_layer':\n",
    "                    inputs=self.form_fc_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='activation_layer':\n",
    "                    inputs=self.form_activation_layer(inputs)\n",
    "                elif layer_params['layer_type']=='pooling_layer':\n",
    "                    inputs=self.form_max_pooling_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='flattening_layer':\n",
    "                    inputs=tf.contrib.layers.flatten(inputs)\n",
    "\n",
    "    #         making logits layer (final output layer)\n",
    "            self.logits=tf.layers.dense(inputs,self.params.num_outputs,activation=None,kernel_initializer=self.params.initializer_fn())\n",
    "\n",
    "            self.loss=self.form_loss(self.logits,self.Y)\n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.train_op=optimizer.minimize(loss = self.loss,global_step=self.step_no)\n",
    "            \n",
    "            self.initializer=tf.global_variables_initializer()\n",
    "        model_variables=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.params.name_scope)#saving only the varuiables belonging to this scope\n",
    "        self.saver=tf.train.Saver(var_list=model_variables)\n",
    "        \n",
    "    def save_model(self,sess,savedir=\"/\",step=0):\n",
    "        savedir=os.path.join(os.getcwd(),savedir)\n",
    "        savedir=os.path.join(savedir,self.params.name_scope)\n",
    "        if not hasattr(self,'saved_before'):#calling save model for the first time\n",
    "            if not os.path.isdir(savedir):#creating directory if not exists\n",
    "                try:  \n",
    "                    os.makedirs(savedir)\n",
    "                except OSError:\n",
    "                    print ('failed to make the specified_directory.Returning...')\n",
    "                    return\n",
    "            file_pi = open(os.path.join(savedir,\"model_object.pkl\"), 'wb+') #saving param object\n",
    "            pickle.dump(self.params, file_pi)\n",
    "            #saving tensorflow graph and weight values\n",
    "#             path=os.path.join(savedir,(self.params.name_scope+\".ckpt\"))\n",
    "            print (\"saving path:{}\".format(str(savedir)))\n",
    "            self.saver.save(sess,os.path.join(savedir,\"model_weights.ckpt\"), global_step=step) #saving model weights\n",
    "            self.saved_before=True\n",
    "        else:    #saving model weights\n",
    "            self.saver.save(sess,os.path.join(savedir,\"model_weights.ckpt\"), global_step=step,write_meta_graph=False)#writes meta graph for the first time save_model is called\n",
    "    def restore_params_fn(self,pickle_file_path):\n",
    "        if os.path.exists(pickle_file_path):\n",
    "            filehandler = open(pickle_file_path, 'rb')\n",
    "            self.params=pickle.load(filehandler)\n",
    "        else:\n",
    "            print(\"no such file exists\")\n",
    "        \n",
    "    def restore_model(self,sess,restore_dir):\n",
    "        restore_dir=os.path.join(os.getcwd(),restore_dir)\n",
    "#         restore_dir=os.path.join(restore_dir,\"\\\\\")\n",
    "#         path=restore_dir\n",
    "        restore_dir=os.path.join(restore_dir,self.params.name_scope)\n",
    "        print (\"restoring path:{}\".format(str(restore_dir)))\n",
    "#         print (path)\n",
    "        self.saver.restore(sess, tf.train.latest_checkpoint(restore_dir))#loading latest model\n",
    "        \n",
    "    def train(self,sess,n_epochs,get_next_batch_fn,save_every_n_iter,log_every_n_iter,save_dir,initialize=False):\n",
    "        if initialize:\n",
    "            sess.run([self.initializer])\n",
    "        for epoch in np.arange(n_epochs):\n",
    "            print(\"----Epoch=\"+str(epoch)+\"\\n\")\n",
    "            for x,y in get_next_batch_fn():\n",
    "                feed_dict={self.X:x,self.Y:y,self.lr_placeholder:self.params.learning_rate}\n",
    "                loss,step_no,_=sess.run([self.loss,self.step_no,self.train_op],feed_dict=feed_dict)\n",
    "\n",
    "                if (step_no)%save_every_n_iter==0:\n",
    "                    print(\"saving model\\n\")\n",
    "                    self.save_model(sess,save_dir,self.step_no)\n",
    "#                 print (\"\"\"loss= \"+str(loss)+\"\\n\")\n",
    "                print (\"Step={} and loss occured= {} \\n\".format(str(step_no),str(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'input_shape':[None, 35, 190, 1],\n",
    "    'num_outputs':3,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':1,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':1,'num_filters':16,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':256},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':100},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.relu,\n",
    "    'loss_fn':tf.losses.softmax_cross_entropy,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'neural_network_v1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing model \n",
    "# tf.reset_default_graph()\n",
    "# model=CNN_Model(params)\n",
    "# model.build_model()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(model.initializer)\n",
    "#     model.save_model(sess,\"1st_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing restoring a new model\n",
    "# tf.reset_default_graph()\n",
    "# model=CNN_Model(restore_params=True,pickle_file_path=\"1st_try/neural_network_v1.pkl\")\n",
    "# model.build_model()\n",
    "# with tf.Session() as sess:\n",
    "#     model.restore_model(sess,\"1st_try\")\n",
    "# #     sess.run(model.initializer)\n",
    "# #     model.save_model(sess,\"1st_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=120\n",
    "data=Data('training_data.npy',batch_size,load_directly=True,X_data_path=\"data_X.npy\",Y_data_path=\"data_Y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring path:/home/armughan/Desktop/machine learning/codes/Chrome_dino_game/AI_Plays_Dino_Game/1st_try/neural_network_v1\n",
      "INFO:tensorflow:Restoring parameters from /home/armughan/Desktop/machine learning/codes/Chrome_dino_game/AI_Plays_Dino_Game/1st_try/neural_network_v1/model_weights.ckpt-10\n",
      "----Epoch=0\n",
      "\n",
      "Step=11.0 and loss occured= 1.076945 \n",
      "\n",
      "Step=12.0 and loss occured= 1.066438 \n",
      "\n",
      "Step=13.0 and loss occured= 1.06242 \n",
      "\n",
      "Step=14.0 and loss occured= 1.0614237 \n",
      "\n",
      "saving model\n",
      "\n",
      "saving path:/home/armughan/Desktop/machine learning/codes/Chrome_dino_game/AI_Plays_Dino_Game/1st_try/neural_network_v1\n",
      "Step=15.0 and loss occured= 1.0580553 \n",
      "\n",
      "Step=16.0 and loss occured= 1.0488509 \n",
      "\n",
      "Step=17.0 and loss occured= 1.0423756 \n",
      "\n",
      "Step=18.0 and loss occured= 1.0378089 \n",
      "\n",
      "Step=19.0 and loss occured= 1.0401565 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e1e911dbd570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     sess.run(model.initializer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#     model.save_model(sess,\"1st_try\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#     for x,y in data.get_next_batch():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#         print (x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-492f7fc2ea75>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, n_epochs, get_next_batch_fn, save_every_n_iter, log_every_n_iter, save_dir, initialize)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_next_batch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=50\n",
    "n_iter=int(data.X_train.shape[0]/data.batch_size)\n",
    "save_every_n_iter=5\n",
    "log_every_n_iter=20\n",
    "initialize=False\n",
    "save_dir=\"1st_try\"\n",
    "tf.reset_default_graph()\n",
    "# model=CNN_Model(params)\n",
    "\n",
    "    \n",
    "\n",
    "model=\"\"\n",
    "with tf.Session() as sess:\n",
    "    if(not initialize):\n",
    "        model=CNN_Model(restore_params=True,pickle_file_path=\"1st_try/neural_network_v1/model_object.pkl\")\n",
    "        model.build_model()\n",
    "        model.restore_model(sess,save_dir)\n",
    "#         model.params.learning_rate=0.0003\n",
    "    else:\n",
    "        model=CNN_Model(params)\n",
    "        model.build_model()\n",
    "#         model.params.step_no=765\n",
    "#     sess.run(model.initializer)\n",
    "#     model.save_model(sess,\"1st_try\")\n",
    "    model.train(sess,n_epochs,data.get_next_batch,save_every_n_iter,log_every_n_iter,save_dir=save_dir,initialize=initialize)\n",
    "#     for x,y in data.get_next_batch():\n",
    "#         print (x.shape)\n",
    "#         print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
