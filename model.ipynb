{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/armughan/anaconda3/envs/py3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/armughan/anaconda3/envs/py3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():#used to store parameter values\n",
    "    def __init__(self,params):\n",
    "        #STORING PARAMETER VALUES \n",
    "        self.input_shape=params['input_shape']\n",
    "        self.num_outputs=params['num_outputs']\n",
    "        self.layer_hierarchy=params['layer_hierarchy']\n",
    "        self.activation_fn=params.get('activation_fn',tf.nn.relu)\n",
    "        self.loss_fn=params.get('loss_fn',tf.losses.softmax_cross_entropy)\n",
    "        self.learning_rate=params['learning_rate']\n",
    "        self.optimizer_fn=params['optimizer_fn']\n",
    "        self.initializer_fn=params['initializer_fn']\n",
    "        self.name_scope=params['name_scope']\n",
    "#         self.step_no=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model():\n",
    "    def __init__(self,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        #STORING PARAMETER VALUES\n",
    "        if not restore_params:\n",
    "            self.params=Params(param_dict)\n",
    "        else:\n",
    "            self.restore_params_fn(pickle_file_path)\n",
    "\n",
    "    def form_placeholder(self,shape,dt=tf.float32):\n",
    "        X=tf.placeholder(dt,shape=shape)\n",
    "        return X\n",
    "    def form_variable(self,shape,dt=tf.float32,name=\"\",trainable=True,initializer=tf.zeros_initializer):\n",
    "        if name==\"\":\n",
    "            return tf.Variable(initial_value=initializer,trainable=trainable,dtype=dt)\n",
    "        else:\n",
    "#             initializer=tf.constant_initializer(initail_val)\n",
    "            return tf.get_variable(name=name,shape=shape, dtype=dt,initializer=initializer(),trainable=trainable)\n",
    "    \n",
    "    def form_convolutional_layer(self,inputs,layer_params):\n",
    "        return tf.layers.conv2d(\n",
    "                    inputs=inputs,\n",
    "                    filters=layer_params['num_filters'],\n",
    "                    kernel_size=layer_params['kernel_size'],\n",
    "                    strides=layer_params['kernel_strides'],\n",
    "                    padding=layer_params['padding'],\n",
    "                    kernel_initializer=self.params.initializer_fn(),\n",
    "                    activation=None)\n",
    "    def form_max_pooling_layer(self,inputs,layer_params):\n",
    "        tf.layers.max_pooling2d(\n",
    "                    inputs=inputs,\n",
    "                    pool_size=layer_params['pool_size'],\n",
    "                    strides=layer_params['pool_strides'])\n",
    "    def form_activation_layer(self,inputs):\n",
    "        return self.params.activation_fn(inputs)\n",
    "    \n",
    "    def form_fc_layer(self,inputs,layer_params):\n",
    "        return tf.layers.dense(inputs,layer_params['num_hidden_units'],activation=None,kernel_initializer=self.params.initializer_fn())\n",
    "    \n",
    "    def form_batch_normalization_layer(self,inputs):\n",
    "        return tf.layers.batch_normalization(inputs=inputs,axis=-1,training=self.training_mode)\n",
    "    \n",
    "    def form_dropout_layer(self,inputs,layer_params):\n",
    "        dropout_probability=layer_params.get('dropout_probability',0.5)\n",
    "        noise_shape=layer_params.get('dropout_mask_shape',None)\n",
    "        return tf.layers.dropout(inputs,rate=dropout_probability,noise_shape=noise_shape,training=self.training_mode)\n",
    "    \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(onehot_labels=targets,logits=logits,reduction=tf.losses.Reduction.NONE)\n",
    "        return entropies\n",
    "    \n",
    "    def build_model_till_logits(self):\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            self.X=self.form_placeholder(self.params.input_shape)\n",
    "            self.lr_placeholder=self.form_placeholder([]) #since we can change learning arate during training\n",
    "            self.training_mode=self.form_placeholder([],tf.bool)\n",
    "            self.step_no=self.form_variable(shape=[],name=\"step_no\",trainable=False)#stores number of steps for which training has occured\n",
    "            self.epoch_no=self.form_variable(shape=[],name=\"epoch_no\",trainable=False) #stores number of epochs for which training is performed\n",
    "\n",
    "            inputs=self.X\n",
    "            for layer_params in self.params.layer_hierarchy:\n",
    "                if layer_params['layer_type']=='conv_layer':\n",
    "                    inputs=self.form_convolutional_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='fc_layer':\n",
    "                    inputs=self.form_fc_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='activation_layer':\n",
    "                    inputs=self.form_activation_layer(inputs)\n",
    "                elif layer_params['layer_type']=='pooling_layer':\n",
    "                    inputs=self.form_max_pooling_layer(inputs,layer_params)\n",
    "                elif layer_params['layer_type']=='flattening_layer':\n",
    "                    inputs=tf.contrib.layers.flatten(inputs)\n",
    "                elif layer_params['layer_type']=='batch_normalization_layer':\n",
    "                    inputs=self.form_batch_normalization_layer(inputs)\n",
    "                elif layer_params['layer_type']=='dropout_layer':\n",
    "                    inputs=self.form_dropout_layer(inputs,layer_params)\n",
    "\n",
    "    #         making logits layer (final output layer)\n",
    "            self.logits=tf.layers.dense(inputs,self.params.num_outputs,activation=None,kernel_initializer=self.params.initializer_fn())\n",
    "            \n",
    "            self.model_variables=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.params.name_scope)#saving only the varuiables belonging to this scope\n",
    "            self.saver=tf.train.Saver(var_list=self.model_variables)\n",
    "            self.increment_epoch_op=tf.assign(self.epoch_no, self.epoch_no+1)#op to update number of epoch by + 1\n",
    "\n",
    "    def Build_model(self):\n",
    "        self.build_model_till_logits()\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            \n",
    "            self.Y=self.form_placeholder((None,self.params.num_outputs),tf.float32)\n",
    "            self.loss=tf.reduce_mean(self.form_loss(self.logits,self.Y))            \n",
    "\n",
    "            self.predictions=tf.argmax(tf.nn.softmax(self.logits),1)\n",
    "            equality = tf.equal(self.predictions,tf.argmax(self.Y,1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "\n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.grads_and_vars=optimizer.compute_gradients(loss=self.loss,var_list=self.model_variables)\n",
    "            self.train_op=optimizer.apply_gradients(grads_and_vars=self.grads_and_vars,global_step=self.step_no)\n",
    "#             self.train_op=optimizer.minimize(loss = self.loss,global_step=self.step_no)\n",
    "\n",
    "\n",
    "            #summary ops\n",
    "            loss_summary=tf.summary.scalar(\"loss\",self.loss)\n",
    "            acc_summary=tf.summary.scalar(\"accuracy\",self.accuracy)\n",
    "            #             self.summaries=tf.summary.merge_all(scope=self.params.name_scope)\n",
    "            self.summaries=tf.summary.merge([loss_summary,acc_summary])\n",
    "            self.initializer=tf.global_variables_initializer()\n",
    "    def create_log_directory_if_doesnt_exist(self,savedir):\n",
    "        savedir=os.path.join(os.getcwd(),savedir)\n",
    "        savedir=os.path.join(savedir,self.params.name_scope)\n",
    "        savedir=os.path.join(savedir,\"logs\")\n",
    "        if not os.path.isdir(savedir):#creating directory if not exists\n",
    "            try:  \n",
    "                os.makedirs(savedir)\n",
    "                return savedir,True\n",
    "            except OSError:\n",
    "                print ('failed to make the specified_directory.Returning...')\n",
    "                return \"\",False\n",
    "        return savedir,True\n",
    "            \n",
    "        \n",
    "    def save_model(self,sess,savedir=\"/\",step=0):\n",
    "        savedir=os.path.join(os.getcwd(),savedir)\n",
    "        savedir=os.path.join(savedir,self.params.name_scope)\n",
    "        if not hasattr(self,'saved_before'):#calling save model for the first time\n",
    "            if not os.path.isdir(savedir):#creating directory if not exists\n",
    "                try:  \n",
    "                    os.makedirs(savedir)\n",
    "                except OSError:\n",
    "                    print ('failed to make the specified_directory.Returning...')\n",
    "                    return\n",
    "            file_pi = open(os.path.join(savedir,\"model_object.pkl\"), 'wb+') #saving param object\n",
    "            pickle.dump(self.params, file_pi)\n",
    "            #saving tensorflow graph and weight values\n",
    "#             path=os.path.join(savedir,(self.params.name_scope+\".ckpt\"))\n",
    "#             print (\"saving path:{}\".format(str(savedir)))\n",
    "            self.saver.save(sess,os.path.join(savedir,\"model_weights.ckpt\"), global_step=step) #saving model weights\n",
    "            self.saved_before=True\n",
    "        else:    #saving model weights\n",
    "            self.saver.save(sess,os.path.join(savedir,\"model_weights.ckpt\"), global_step=step,write_meta_graph=False)#writes meta graph for the first time save_model is called\n",
    "    def restore_params_fn(self,pickle_file_path):\n",
    "        if os.path.exists(pickle_file_path):\n",
    "            filehandler = open(pickle_file_path, 'rb')\n",
    "            self.params=pickle.load(filehandler)\n",
    "        else:\n",
    "            print(\"no such file exists\")\n",
    "        \n",
    "    def restore_model(self,sess,restore_dir):\n",
    "        restore_dir=os.path.join(os.getcwd(),restore_dir)\n",
    "#         restore_dir=os.path.join(restore_dir,\"\\\\\")\n",
    "#         path=restore_dir\n",
    "        restore_dir=os.path.join(restore_dir,self.params.name_scope)\n",
    "        print (\"restoring path:{}\".format(str(restore_dir)))\n",
    "#         print (path)\n",
    "        self.saver.restore(sess, tf.train.latest_checkpoint(restore_dir))#loading latest model\n",
    "         \n",
    "    def train(self,sess,n_epochs,get_next_batch_fn,get_validation_set_fn,save_every_n_iter,log_train_every_n_iter,log_validation_every_n_iter,save_dir,initialize=False,set_logging=True):\n",
    "        if initialize:\n",
    "            sess.run([self.initializer])\n",
    "        if set_logging:\n",
    "            log_dir,set_logging=self.create_log_directory_if_doesnt_exist(save_dir)\n",
    "        if set_logging: #creating file handlers if dir cretaed or found in above statement\n",
    "            train_writer = tf.summary.FileWriter(os.path.join(log_dir,'train'), sess.graph)\n",
    "            validation_writer = tf.summary.FileWriter(os.path.join(log_dir ,'validation'))\n",
    "        [step_no]=sess.run([self.step_no]) \n",
    "        [epoch]=sess.run([self.epoch_no])\n",
    "        ending_epoch=n_epochs+epoch\n",
    "        while epoch < ending_epoch:\n",
    "            print(\"----Epoch=\"+str(epoch)+\"\\n\")\n",
    "            for x,y in get_next_batch_fn():\n",
    "                feed_dict={self.X:x,self.Y:y,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                if step_no%log_train_every_n_iter==0 and set_logging:\n",
    "                    summaries,loss,acc,new_step_no,_=sess.run([self.summaries,self.loss,self.accuracy,self.step_no,self.train_op],feed_dict=feed_dict)\n",
    "                    train_writer.add_summary(summaries,step_no)\n",
    "                else:\n",
    "                    loss,acc,new_step_no,_=sess.run([self.loss,self.accuracy,self.step_no,self.train_op],feed_dict=feed_dict)\n",
    "                print (\"Step={} and loss occured= {} and acc= {} \\n\".format(str(step_no),str(loss),str(acc)))\n",
    "                feed_dict=None #freeing memory\n",
    "                x=y=None\n",
    "#                 if step_no%log_validation_every_n_iter==0 and set_logging:\n",
    "#                     x,y=get_validation_set_fn()\n",
    "#                     feed_dict={self.X:x,self.Y:y,self.training_mode:False}\n",
    "#                     [summaries]=sess.run([self.summaries],feed_dict=feed_dict)\n",
    "#                     validation_writer.add_summary(summaries, step_no)\n",
    "\n",
    "                if (step_no)%save_every_n_iter==0:\n",
    "                    print(\"saving model\\n\")\n",
    "                    self.save_model(sess,save_dir,self.step_no)\n",
    "                step_no=new_step_no\n",
    "#                 print (\"\"\"loss= \"+str(loss)+\"\\n\")\n",
    "            _,epoch=sess.run([self.increment_epoch_op,self.epoch_no])\n",
    "    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'input_shape':[None, 35, 190, 1],\n",
    "    'num_outputs':3,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':1,'num_filters':16,'padding':'valid'},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':1,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':256},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'dropout_layer','dropout_probability':0.5},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':100},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'dropout_layer','dropout_probability':0.5}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.relu,\n",
    "    'loss_fn':tf.losses.softmax_cross_entropy,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'neural_network_bn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing model \n",
    "# tf.reset_default_graph()\n",
    "# model=CNN_Model(params)\n",
    "# model.build_model()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(model.initializer)\n",
    "#     model.save_model(sess,\"1st_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing restoring a new model\n",
    "# tf.reset_default_graph()\n",
    "# model=CNN_Model(restore_params=True,pickle_file_path=\"1st_try/neural_network_v1.pkl\")\n",
    "# model.build_model()\n",
    "# with tf.Session() as sess:\n",
    "#     model.restore_model(sess,\"1st_try\")\n",
    "# #     sess.run(model.initializer)\n",
    "# #     model.save_model(sess,\"1st_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating mean and std over training set\n",
      "\n",
      "{'X_train shape': (7273, 35, 190, 1), 'Y_train shape': (7273, 3), 'X_validation shape': (200, 35, 190, 1), 'Y_validation shape': (200, 3), 'X_test shape': (2491, 35, 190, 1), 'Y_test shape': (2491, 3)}\n"
     ]
    }
   ],
   "source": [
    "batch_size=120\n",
    "data=Data(data_path='training_data.npy',batch_size=batch_size,load_directly=True,X_data_path=\"data_X.npy\",Y_data_path=\"data_Y.npy\")\n",
    "print (data.get_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Epoch=0.0\n",
      "\n",
      "Step=0.0 and loss occured= 1.3891732 and acc= 0.425 \n",
      "\n",
      "saving model\n",
      "\n",
      "Step=1.0 and loss occured= 1.1913142 and acc= 0.46666667 \n",
      "\n",
      "Step=2.0 and loss occured= 1.170196 and acc= 0.49166667 \n",
      "\n",
      "Step=3.0 and loss occured= 1.1629349 and acc= 0.49166667 \n",
      "\n",
      "Step=4.0 and loss occured= 1.2237021 and acc= 0.55833334 \n",
      "\n",
      "Step=5.0 and loss occured= 1.0482911 and acc= 0.64166665 \n",
      "\n",
      "Step=6.0 and loss occured= 1.0001882 and acc= 0.64166665 \n",
      "\n",
      "Step=7.0 and loss occured= 1.1583576 and acc= 0.53333336 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-49935c2395e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     model.save_model(sess,\"1st_try\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_validation_set_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_train_every_n_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_validation_every_n_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset_logging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#     for x,y in data.get_next_batch():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#         print (x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-760d53a5596a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, n_epochs, get_next_batch_fn, get_validation_set_fn, save_every_n_iter, log_train_every_n_iter, log_validation_every_n_iter, save_dir, initialize, set_logging)\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_step_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Step={} and loss occured= {} and acc= {} \\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;31m#freeing memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=50\n",
    "n_iter=int(data.X_train.shape[0]/data.batch_size)\n",
    "save_every_n_iter=10\n",
    "log_train_every_n_iter=5\n",
    "log_validation_every_n_iter=10\n",
    "initialize=True\n",
    "save_dir=\"another_try\"\n",
    "tf.reset_default_graph()\n",
    "# model=CNN_Model(params)\n",
    "\n",
    "    \n",
    "\n",
    "model=\"\"\n",
    "with tf.Session() as sess:\n",
    "    if(not initialize):\n",
    "        model=CNN_Model(restore_params=True,pickle_file_path=\"1st_try/neural_network_v1/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "        model.restore_model(sess,save_dir)\n",
    "        model.params.learning_rate=0.001\n",
    "    else:\n",
    "        model=CNN_Model(params)\n",
    "        model.Build_model()\n",
    "#         model.params.step_no=765\n",
    "#         sess.run(model.initializer)\n",
    "#     model.save_model(sess,\"1st_try\")\n",
    "\n",
    "    model.train(sess,n_epochs,data.get_next_batch,data.get_validation_set_batch,save_every_n_iter,log_train_every_n_iter,log_validation_every_n_iter,save_dir=save_dir,initialize=initialize,set_logging=True)\n",
    "#     for x,y in data.get_next_batch():\n",
    "#         print (x.shape)\n",
    "#         print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
