{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armughan.Shahid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "from Env import Env\n",
    "from q_network import Q_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting Environment...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\armughan.shahid\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env=Env('SpaceInvaders-v0',convert_to_grayscale=True,crop=True,valid_Y=[20,-10],valid_X=[10,-10],resize=True,resize_Y=90,resize_X=70,normalize=True,num_of_frames_per_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copy_weight_ops(from_network,to_network,tau):\n",
    "    tau=tf.placeholder(tf.float32,[])\n",
    "    op_holder = []\n",
    "    for f_var,t_var in zip(from_network.model_trainable_variables,to_network.model_trainable_variables):\n",
    "        value=(f_var.value()*tau)+((1-tau)*t_var.value())\n",
    "        op_holder.append(t_var.assign(value))\n",
    "    return tau,op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_double_deep_Q(main_Q_network,target_Q_network,sess,episodes,steps,initial_epsilon,final_epsilon,epsilon_dec,\n",
    "                              train_t,discount_rate,batch_size,env,save_dir,save_every_n_iter,log_every_n_iter,\n",
    "                              initialize=False,set_logging=True,num_frames_to_repeat_action=4,train_main_every_n_steps=5,\n",
    "                              update_target_every_n_iters=100,tau=0.001):\n",
    "    tau_placeholder,copy_ops=make_copy_weight_ops(main_Q_network,target_Q_network,tau)\n",
    "    if initialize:\n",
    "        print (\"Initializing.....\\n\")\n",
    "        sess.run([main_Q_network.initializer])\n",
    "    sess.run([target_Q_network.initializer])\n",
    "    sess.run(copy_ops,feed_dict={tau_placeholder:1.0})\n",
    "#     else:\n",
    "        \n",
    "    if set_logging:\n",
    "        print (\"Setting up for Logging ...\\n\")\n",
    "        log_dir,set_logging=main_Q_network.create_log_directory_if_doesnt_exist(save_dir)\n",
    "    if set_logging: #creating file handlers if dir cretaed or found in above statement\n",
    "        print(\"Logging called but no code implemented\")\n",
    "#                 train_writer = tf.summary.FileWriter(os.path.join(log_dir,'train'), sess.graph)\n",
    "#                 validation_writer = tf.summary.FileWriter(os.path.join(log_dir ,'validation'))\n",
    "    print (\"Retreiveing step no...\\n\")\n",
    "    [iter_no]=sess.run([main_Q_network.step_no]) \n",
    "    epsilon=initial_epsilon\n",
    "    for episode in np.arange(episodes):\n",
    "        state=env.reset()\n",
    "        step=0\n",
    "        episode_reward=0\n",
    "        episode_loss=0\n",
    "        previous_action=None\n",
    "        for step in np.arange(steps):\n",
    "            #choosing action \n",
    "            if step%num_frames_to_repeat_action==0:\n",
    "                if epsilon>final_epsilon and iter_no>train_t:\n",
    "                        epsilon-=epsilon_dec\n",
    "\n",
    "#                 if  iter_no<train_t or (np.random.random(1)<epsilon):\n",
    "                if  (np.random.random(1)<epsilon):\n",
    "                    action=np.random.randint(low=0,high=main_Q_network.params.num_outputs,size=1,dtype=np.int32)\n",
    "                else:\n",
    "                    feed_dict={main_Q_network.X:np.expand_dims(state,axis=0),main_Q_network.lr_placeholder:main_Q_network.params.learning_rate,main_Q_network.training_mode:True}\n",
    "                    [action]=sess.run([main_Q_network.max_q_value_actions],feed_dict=feed_dict)\n",
    "                action=np.squeeze(action)\n",
    "            else:\n",
    "                action=previous_action\n",
    "#                 print(action)\n",
    "            next_state,reward,done,info=env.step(action)\n",
    "            episode_reward+=reward\n",
    "            main_Q_network.add_to_experience_replay(state,action,next_state,reward,done)\n",
    "            previous_action=action\n",
    "            episode_has_finished=done\n",
    "\n",
    "            state=next_state\n",
    "\n",
    "            if (main_Q_network.experience_replay_buffer.num_items>train_t )and (step%train_main_every_n_steps==0): #perform training if there are enough experiences\n",
    "#                     print(\"buffer filled\")\n",
    "\n",
    "\n",
    "                #performing training step\n",
    "                states,actions,next_states,rewards,dones=main_Q_network.experience_replay_buffer.get_batch(batch_size=batch_size)\n",
    "\n",
    "\n",
    "                #finding vals of next states\n",
    "#                     print (next_states.shape)\n",
    "                feed_dict={target_Q_network.X:next_states,\n",
    "                           target_Q_network.lr_placeholder:target_Q_network.params.learning_rate,\n",
    "                           target_Q_network.training_mode:True}\n",
    "                [max_q_vals_next_state]=sess.run([target_Q_network.max_q_values],feed_dict=feed_dict)\n",
    "\n",
    "                feed_dict={main_Q_network.X:states,main_Q_network.actions:actions,\n",
    "                           main_Q_network.max_q_values_next_state:max_q_vals_next_state,main_Q_network.rewards:rewards,\n",
    "                           main_Q_network.notended:((np.logical_not(dones)).astype(np.int32)),\n",
    "                           main_Q_network.discount_rate:discount_rate,main_Q_network.lr_placeholder:main_Q_network.params.learning_rate,\n",
    "                           main_Q_network.training_mode:True}\n",
    "                loss,_=sess.run([main_Q_network.loss,main_Q_network.train_op],feed_dict=feed_dict)\n",
    "                episode_loss+=loss\n",
    "                iter_no+=1\n",
    "                if (iter_no)%update_target_every_n_iters==0:\n",
    "                    print(\"updating weights of target network\\n\")\n",
    "                    sess.run(copy_ops,feed_dict={tau_placeholder:tau})\n",
    "                \n",
    "                if (iter_no)%save_every_n_iter==0:\n",
    "                    print(\"^^^^ saving model ^^^^ \\n\")\n",
    "                    main_Q_network.save_model(sess,save_dir,main_Q_network.step_no)\n",
    "                if (iter_no)%log_every_n_iter==0:\n",
    "                    print (\"Trainaing Step:\\t Iteration no={} Game Step ={} loss={} \".format(iter_no,step,loss))\n",
    "            if episode_has_finished:\n",
    "                break\n",
    "        print (\"===================>Episode {} Ended <===================\\n\".format(episode)) \n",
    "        print (\"=======>\\t Episode Length={} \\t<=======\\n\".format(step))   \n",
    "        print (\"=======>\\t Episode Reward={} \\t<=======\\n\".format(episode_reward))\n",
    "        print (\"=======>\\t Mean Episode Loss={} \\t<=======\\n\".format(episode_loss/step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_q_network_params={\n",
    "    'input_shape':[None, *env.image_shape],\n",
    "    'num_outputs':env.action_space,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':4,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':3,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':512},\n",
    "        {'layer_type':'activation_layer'}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.elu,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'main_q_network_with_frames'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_network_params={\n",
    "    'input_shape':[None, *env.image_shape],\n",
    "    'num_outputs':env.action_space,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':4,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':3,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':512},\n",
    "        {'layer_type':'activation_layer'}\n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.elu,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'target_q_network_with_frames'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring path:D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\main_q_network_with_frames\n",
      "INFO:tensorflow:Restoring parameters from D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\main_q_network_with_frames\\model_weights.ckpt-9050\n",
      "Setting up for Logging ...\n",
      "\n",
      "Logging called but no code implemented\n",
      "Retreiveing step no...\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 0 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=942 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=195.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=50.0 Game Step =156 loss=2.525214672088623 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=100.0 Game Step =256 loss=2.344151258468628 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=150.0 Game Step =356 loss=1.2328190803527832 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=200.0 Game Step =456 loss=0.4041798710823059 \n",
      "===================>Episode 1 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=543 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=125.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.674415493675578 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=250.0 Game Step =12 loss=0.2400512844324112 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=300.0 Game Step =112 loss=0.4012090265750885 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=350.0 Game Step =212 loss=0.4581027925014496 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=400.0 Game Step =312 loss=0.20656871795654297 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=450.0 Game Step =412 loss=1.096289873123169 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=500.0 Game Step =512 loss=0.43473827838897705 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=550.0 Game Step =612 loss=0.8796645998954773 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=600.0 Game Step =712 loss=0.18822792172431946 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=650.0 Game Step =812 loss=0.3322891592979431 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=700.0 Game Step =912 loss=0.10459652543067932 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=750.0 Game Step =1012 loss=0.26030343770980835 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=800.0 Game Step =1112 loss=0.23284310102462769 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=850.0 Game Step =1212 loss=0.0947297215461731 \n",
      "===================>Episode 2 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1235 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=320.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.28331842922609346 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=900.0 Game Step =76 loss=0.35002437233924866 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=950.0 Game Step =176 loss=0.14571768045425415 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1000.0 Game Step =276 loss=0.36114490032196045 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1050.0 Game Step =376 loss=0.11409281939268112 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1100.0 Game Step =476 loss=0.12174898386001587 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1150.0 Game Step =576 loss=0.1391363888978958 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1200.0 Game Step =676 loss=0.22937433421611786 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1250.0 Game Step =776 loss=0.20638279616832733 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1300.0 Game Step =876 loss=0.12290483713150024 \n",
      "===================>Episode 3 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=903 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=205.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.19901960815075095 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1350.0 Game Step =72 loss=0.1520290970802307 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1400.0 Game Step =172 loss=0.2265414446592331 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1450.0 Game Step =272 loss=0.30227506160736084 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1500.0 Game Step =372 loss=0.06692726910114288 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1550.0 Game Step =472 loss=4.489627838134766 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1600.0 Game Step =572 loss=4.619019508361816 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1650.0 Game Step =672 loss=0.6089629530906677 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1700.0 Game Step =772 loss=0.2286543995141983 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1750.0 Game Step =872 loss=0.5170196890830994 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1800.0 Game Step =972 loss=0.10519137233495712 \n",
      "===================>Episode 4 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1029 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=200.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2139814639766316 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1850.0 Game Step =42 loss=0.0657305121421814 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1900.0 Game Step =142 loss=0.2663489878177643 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1950.0 Game Step =242 loss=0.16841629147529602 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2000.0 Game Step =342 loss=0.8934444189071655 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2050.0 Game Step =442 loss=0.24124206602573395 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2100.0 Game Step =542 loss=0.231336310505867 \n",
      "===================>Episode 5 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=641 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=105.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.19742630537186695 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2150.0 Game Step =0 loss=0.1282728910446167 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2200.0 Game Step =100 loss=0.18041495978832245 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2250.0 Game Step =200 loss=0.34599682688713074 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2300.0 Game Step =300 loss=0.10046849399805069 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2350.0 Game Step =400 loss=0.26161476969718933 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2400.0 Game Step =500 loss=0.22652608156204224 \n",
      "===================>Episode 6 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=512 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=15.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.21308578430034686 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2450.0 Game Step =86 loss=0.08451274782419205 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2500.0 Game Step =186 loss=0.18977518379688263 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2550.0 Game Step =286 loss=0.13506941497325897 \n",
      "===================>Episode 7 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=382 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=30.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.20999756068264314 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2600.0 Game Step =2 loss=0.045354824513196945 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2650.0 Game Step =102 loss=0.055855512619018555 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2700.0 Game Step =202 loss=0.14669682085514069 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2750.0 Game Step =302 loss=0.1190917044878006 \n",
      "===================>Episode 8 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=363 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=35.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.16681877108029097 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2800.0 Game Step =38 loss=0.1607825607061386 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2850.0 Game Step =138 loss=0.27594706416130066 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2900.0 Game Step =238 loss=0.11351223289966583 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=2950.0 Game Step =338 loss=0.1257755011320114 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3000.0 Game Step =438 loss=0.23446805775165558 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3050.0 Game Step =538 loss=0.22875109314918518 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3100.0 Game Step =638 loss=1.2766109704971313 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3150.0 Game Step =738 loss=0.13313782215118408 \n",
      "===================>Episode 9 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=809 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=135.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.18851098698898874 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3200.0 Game Step =28 loss=0.1087769865989685 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3250.0 Game Step =128 loss=0.11601634323596954 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3300.0 Game Step =228 loss=0.1451154351234436 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3350.0 Game Step =328 loss=0.20985530316829681 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3400.0 Game Step =428 loss=0.4460271894931793 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3450.0 Game Step =528 loss=0.18172188103199005 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3500.0 Game Step =628 loss=0.12156876921653748 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3550.0 Game Step =728 loss=0.163757786154747 \n",
      "===================>Episode 10 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=806 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=170.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2246005255382314 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3600.0 Game Step =20 loss=1.3240405321121216 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3650.0 Game Step =120 loss=0.37748777866363525 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3700.0 Game Step =220 loss=0.6323139667510986 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3750.0 Game Step =320 loss=0.07907667756080627 \n",
      "===================>Episode 11 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=360 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=100.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2645304705016315 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3800.0 Game Step =58 loss=0.8011240363121033 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3850.0 Game Step =158 loss=0.05465330928564072 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3900.0 Game Step =258 loss=0.2405099868774414 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=3950.0 Game Step =358 loss=0.22175715863704681 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4000.0 Game Step =458 loss=0.3070909380912781 \n",
      "===================>Episode 12 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=545 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=60.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.24663089433121024 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4050.0 Game Step =12 loss=0.09862325340509415 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4100.0 Game Step =112 loss=0.07123922556638718 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4150.0 Game Step =212 loss=0.04417400062084198 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4200.0 Game Step =312 loss=0.0928921028971672 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4250.0 Game Step =412 loss=0.037216152995824814 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4300.0 Game Step =512 loss=0.07446183264255524 \n",
      "===================>Episode 13 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=603 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=125.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2015553556481503 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4350.0 Game Step =8 loss=0.2446093112230301 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4400.0 Game Step =108 loss=0.0998794287443161 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4450.0 Game Step =208 loss=2.4887750148773193 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4500.0 Game Step =308 loss=0.032599035650491714 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4550.0 Game Step =408 loss=0.6680423617362976 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4600.0 Game Step =508 loss=0.3503763973712921 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4650.0 Game Step =608 loss=0.7107195258140564 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4700.0 Game Step =708 loss=0.0735604390501976 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4750.0 Game Step =808 loss=0.07974129915237427 \n",
      "===================>Episode 14 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=847 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=215.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.18061168719887943 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4800.0 Game Step =60 loss=0.042448438704013824 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4850.0 Game Step =160 loss=0.06737227737903595 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4900.0 Game Step =260 loss=0.21212494373321533 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=4950.0 Game Step =360 loss=0.10109318792819977 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5000.0 Game Step =460 loss=0.1360657662153244 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5050.0 Game Step =560 loss=2.3949317932128906 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5100.0 Game Step =660 loss=0.10892163217067719 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5150.0 Game Step =760 loss=0.11276528239250183 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5200.0 Game Step =860 loss=0.2334396243095398 \n",
      "===================>Episode 15 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=912 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=215.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.28884131767738025 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5250.0 Game Step =46 loss=0.08277187496423721 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5300.0 Game Step =146 loss=4.884472846984863 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5350.0 Game Step =246 loss=0.12316753715276718 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5400.0 Game Step =346 loss=0.42467814683914185 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5450.0 Game Step =446 loss=0.7933237552642822 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5500.0 Game Step =546 loss=0.039564408361911774 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5550.0 Game Step =646 loss=0.9801492094993591 \n",
      "===================>Episode 16 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=667 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=120.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.31142834442733286 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5600.0 Game Step =78 loss=0.13832512497901917 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5650.0 Game Step =178 loss=0.4881148040294647 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5700.0 Game Step =278 loss=0.18285545706748962 \n",
      "===================>Episode 17 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=366 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=35.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2170718983222878 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5750.0 Game Step =10 loss=0.04060792550444603 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5800.0 Game Step =110 loss=0.33561110496520996 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5850.0 Game Step =210 loss=0.10348424315452576 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5900.0 Game Step =310 loss=0.05747411772608757 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=5950.0 Game Step =410 loss=0.18737977743148804 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6000.0 Game Step =510 loss=0.0918106958270073 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6050.0 Game Step =610 loss=0.1335660070180893 \n",
      "===================>Episode 18 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=658 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=165.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.21818586674414145 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6100.0 Game Step =50 loss=0.12977512180805206 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6150.0 Game Step =150 loss=0.28666144609451294 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6200.0 Game Step =250 loss=0.28952157497406006 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6250.0 Game Step =350 loss=0.5542562007904053 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6300.0 Game Step =450 loss=0.23819434642791748 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6350.0 Game Step =550 loss=0.17303581535816193 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6400.0 Game Step =650 loss=0.0687025934457779 \n",
      "===================>Episode 19 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=728 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=240.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2359536044187025 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6450.0 Game Step =20 loss=0.6856065988540649 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6500.0 Game Step =120 loss=4.711485385894775 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6550.0 Game Step =220 loss=4.940765857696533 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6600.0 Game Step =320 loss=0.051480576395988464 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6650.0 Game Step =420 loss=0.39396706223487854 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6700.0 Game Step =520 loss=0.02843553014099598 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6750.0 Game Step =620 loss=0.11926309764385223 \n",
      "===================>Episode 20 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=638 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=60.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.26677836269011784 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6800.0 Game Step =80 loss=0.25109055638313293 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6850.0 Game Step =180 loss=0.03471150994300842 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6900.0 Game Step =280 loss=0.07379571348428726 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=6950.0 Game Step =380 loss=0.051190730184316635 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7000.0 Game Step =480 loss=0.18444831669330597 \n",
      "===================>Episode 21 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=499 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=20.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2280294117041843 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7050.0 Game Step =80 loss=0.08485586196184158 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7100.0 Game Step =180 loss=0.11648957431316376 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7150.0 Game Step =280 loss=0.14815010130405426 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7200.0 Game Step =380 loss=0.03587319329380989 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7250.0 Game Step =480 loss=0.049117229878902435 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7300.0 Game Step =580 loss=0.07973314076662064 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7350.0 Game Step =680 loss=0.16791954636573792 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7400.0 Game Step =780 loss=0.13060733675956726 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7450.0 Game Step =880 loss=0.029660064727067947 \n",
      "===================>Episode 22 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=884 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=215.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.1912458945862207 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7500.0 Game Step =94 loss=0.03032015822827816 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7550.0 Game Step =194 loss=0.47656357288360596 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7600.0 Game Step =294 loss=0.07764001935720444 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7650.0 Game Step =394 loss=0.12847572565078735 \n",
      "===================>Episode 23 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=491 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=35.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2585835113302025 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7700.0 Game Step =2 loss=0.4388536214828491 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7750.0 Game Step =102 loss=0.1053587794303894 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7800.0 Game Step =202 loss=0.2114560753107071 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7850.0 Game Step =302 loss=0.6343313455581665 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainaing Step:\t Iteration no=7900.0 Game Step =402 loss=0.2002931535243988 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=7950.0 Game Step =502 loss=0.23837445676326752 \n",
      "===================>Episode 24 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=543 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=20.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.17392410037088898 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8000.0 Game Step =58 loss=0.06148650497198105 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8050.0 Game Step =158 loss=1.925808072090149 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8100.0 Game Step =258 loss=0.09147528558969498 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8150.0 Game Step =358 loss=0.1520327627658844 \n",
      "===================>Episode 25 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=420 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=10.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.11718898790311955 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8200.0 Game Step =36 loss=0.2591240406036377 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8250.0 Game Step =136 loss=0.12816627323627472 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8300.0 Game Step =236 loss=0.039982348680496216 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8350.0 Game Step =336 loss=0.031853340566158295 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8400.0 Game Step =436 loss=0.03243809565901756 \n",
      "===================>Episode 26 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=485 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=25.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.06383523167754264 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8450.0 Game Step =50 loss=0.02393781952559948 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8500.0 Game Step =150 loss=0.08701533079147339 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8550.0 Game Step =250 loss=3.133420705795288 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8600.0 Game Step =350 loss=0.05248556286096573 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8650.0 Game Step =450 loss=0.5789145231246948 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8700.0 Game Step =550 loss=0.06356117129325867 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8750.0 Game Step =650 loss=0.38021767139434814 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8800.0 Game Step =750 loss=0.10584940761327744 \n",
      "===================>Episode 27 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=771 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=390.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=1.1799143394274123 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8850.0 Game Step =78 loss=0.12400338053703308 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8900.0 Game Step =178 loss=0.9079328179359436 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=8950.0 Game Step =278 loss=0.3950243294239044 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9000.0 Game Step =378 loss=0.3472041189670563 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9050.0 Game Step =478 loss=0.5209631323814392 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9100.0 Game Step =578 loss=0.17140546441078186 \n",
      "===================>Episode 28 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=615 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=95.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=1.5101514782968575 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9150.0 Game Step =62 loss=1.2405264377593994 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9200.0 Game Step =162 loss=0.4954407811164856 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9250.0 Game Step =262 loss=0.1242271214723587 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9300.0 Game Step =362 loss=0.5221482515335083 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9350.0 Game Step =462 loss=0.16789788007736206 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9400.0 Game Step =562 loss=1.3894702196121216 \n",
      "===================>Episode 29 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=652 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=90.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=1.747153453497456 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9450.0 Game Step =8 loss=0.4184156358242035 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9500.0 Game Step =108 loss=0.2680831849575043 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9550.0 Game Step =208 loss=0.22435425221920013 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9600.0 Game Step =308 loss=1.0357836484909058 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9650.0 Game Step =408 loss=1.5429298877716064 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9700.0 Game Step =508 loss=0.08022086322307587 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9750.0 Game Step =608 loss=0.3361114263534546 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9800.0 Game Step =708 loss=0.2431163787841797 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9850.0 Game Step =808 loss=0.09189234673976898 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9900.0 Game Step =908 loss=4.258161544799805 \n",
      "===================>Episode 30 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=945 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=195.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.3521130283358236 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=9950.0 Game Step =62 loss=0.10369450598955154 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10000.0 Game Step =162 loss=0.0725262463092804 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10050.0 Game Step =262 loss=0.039254941046237946 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10100.0 Game Step =362 loss=0.5927258729934692 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10150.0 Game Step =462 loss=3.8599865436553955 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10200.0 Game Step =562 loss=0.05646197870373726 \n",
      "===================>Episode 31 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=632 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=125.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2687160207100118 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10250.0 Game Step =28 loss=1.8760244846343994 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10300.0 Game Step =128 loss=3.2810471057891846 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10350.0 Game Step =228 loss=1.2780296802520752 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10400.0 Game Step =328 loss=0.05338893085718155 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10450.0 Game Step =428 loss=1.9503257274627686 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10500.0 Game Step =528 loss=0.10531554371118546 \n",
      "===================>Episode 32 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=615 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=75.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.44680697880988196 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10550.0 Game Step =12 loss=2.0324459075927734 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10600.0 Game Step =112 loss=0.1533958911895752 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10650.0 Game Step =212 loss=0.705711841583252 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10700.0 Game Step =312 loss=0.034255608916282654 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10750.0 Game Step =412 loss=0.500761091709137 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10800.0 Game Step =512 loss=0.06596342474222183 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10850.0 Game Step =612 loss=0.055222999304533005 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10900.0 Game Step =712 loss=0.061354681849479675 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10950.0 Game Step =812 loss=0.03854233771562576 \n",
      "===================>Episode 33 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=907 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=220.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.30343093661038983 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11000.0 Game Step =4 loss=0.1723397970199585 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11050.0 Game Step =104 loss=0.41550976037979126 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11100.0 Game Step =204 loss=0.2662624716758728 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11150.0 Game Step =304 loss=2.7168703079223633 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11200.0 Game Step =404 loss=0.21180501580238342 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11250.0 Game Step =504 loss=0.5964061617851257 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11300.0 Game Step =604 loss=0.27520886063575745 \n",
      "===================>Episode 34 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=665 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=105.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.33183232361548826 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11350.0 Game Step =38 loss=2.3437535762786865 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11400.0 Game Step =138 loss=0.16548380255699158 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11450.0 Game Step =238 loss=0.9073217511177063 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11500.0 Game Step =338 loss=0.9653100371360779 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11550.0 Game Step =438 loss=0.18933700025081635 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11600.0 Game Step =538 loss=1.5297694206237793 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11650.0 Game Step =638 loss=2.9438436031341553 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11700.0 Game Step =738 loss=0.15220333635807037 \n",
      "===================>Episode 35 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=786 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=155.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.3094359095618318 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11750.0 Game Step =50 loss=2.5555734634399414 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11800.0 Game Step =150 loss=0.14507944881916046 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11850.0 Game Step =250 loss=0.30638590455055237 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11900.0 Game Step =350 loss=0.07925938814878464 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=11950.0 Game Step =450 loss=0.07271936535835266 \n",
      "===================>Episode 36 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=541 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=55.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2515170314726239 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12000.0 Game Step =8 loss=0.4230263829231262 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12050.0 Game Step =108 loss=0.3536926209926605 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12100.0 Game Step =208 loss=7.2259955406188965 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12150.0 Game Step =308 loss=0.7332003712654114 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12200.0 Game Step =408 loss=0.09018294513225555 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12250.0 Game Step =508 loss=0.4802875220775604 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12300.0 Game Step =608 loss=0.1513548642396927 \n",
      "===================>Episode 37 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=663 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=135.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.3656193841512836 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12350.0 Game Step =44 loss=1.8571184873580933 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12400.0 Game Step =144 loss=0.5208944082260132 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12450.0 Game Step =244 loss=2.5240986347198486 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12500.0 Game Step =344 loss=0.2319110631942749 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12550.0 Game Step =444 loss=0.06406833976507187 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12600.0 Game Step =544 loss=0.7275731563568115 \n",
      "===================>Episode 38 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=570 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=75.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.4136601251337612 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12650.0 Game Step =72 loss=0.5235697031021118 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12700.0 Game Step =172 loss=0.4990883767604828 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12750.0 Game Step =272 loss=0.774980902671814 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12800.0 Game Step =372 loss=0.32575303316116333 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=12850.0 Game Step =472 loss=0.6587327718734741 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-137c4c6c3ad5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m                               \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                               \u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset_logging\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                               num_frames_to_repeat_action=4,train_main_every_n_steps=2,update_target_every_n_iters=100)\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m#     model.test(initialize=True,env=env)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d15e29834e80>\u001b[0m in \u001b[0;36mtrain_using_double_deep_Q\u001b[1;34m(main_Q_network, target_Q_network, sess, episodes, steps, initial_epsilon, final_epsilon, epsilon_dec, train_t, discount_rate, batch_size, env, save_dir, save_every_n_iter, log_every_n_iter, initialize, set_logging, num_frames_to_repeat_action, train_main_every_n_steps, update_target_every_n_iters, tau)\u001b[0m\n\u001b[0;32m     71\u001b[0m                            \u001b[0mmain_Q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmain_Q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmain_Q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                            main_Q_network.training_mode:True}\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmain_Q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmain_Q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m                 \u001b[0mepisode_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0miter_no\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes=50\n",
    "max_steps=50000\n",
    "save_every_n_iter=50\n",
    "log_every_n_iter=50\n",
    "initialize=False\n",
    "save_dir=\"deep_q_saves\"\n",
    "max_experience_buffer_len=10000\n",
    "initial_epsilon=0.5#1\n",
    "final_epsilon=0.0001\n",
    "epsilon_dec=0.00001\n",
    "train_t=1000\n",
    "discount_rate=0.9\n",
    "batch_size=200\n",
    "pickle_file_path_main_network=\"deep_q_saves/main_q_network_with_frames/model_object.pkl\"\n",
    "pickle_file_path_target_network=\"deep_q_saves/target_q_network_with_frames/model_object.pkl\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "main_Q_network=Q_Network(max_experience_buffer_len,main_q_network_params,restore_params=not initialize,pickle_file_path=pickle_file_path_main_network)\n",
    "target_Q_network=Q_Network(0,target_q_network_params,restore_params=False,pickle_file_path=pickle_file_path_target_network)\n",
    "main_Q_network.Build_model()\n",
    "target_Q_network.Build_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if(not initialize):\n",
    "        main_Q_network.restore_model(sess,save_dir)\n",
    "\n",
    "    \n",
    "    train_using_double_deep_Q(main_Q_network=main_Q_network,target_Q_network=target_Q_network,sess=sess,episodes=n_episodes,\n",
    "                              steps=max_steps,initial_epsilon=initial_epsilon,final_epsilon=final_epsilon,\n",
    "                              epsilon_dec=epsilon_dec,train_t=train_t,discount_rate=discount_rate,batch_size=batch_size,\n",
    "                              env=env,save_dir=save_dir,save_every_n_iter=save_every_n_iter,\n",
    "                              log_every_n_iter=log_every_n_iter,initialize=initialize,set_logging=True,\n",
    "                              num_frames_to_repeat_action=4,train_main_every_n_steps=2,update_target_every_n_iters=100,tau=1)\n",
    "#     model.test(initialize=True,env=env)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring path:D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\main_q_network_with_frames\n",
      "INFO:tensorflow:Restoring parameters from D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\main_q_network_with_frames\\model_weights.ckpt-12850\n",
      "Initializing.....\n",
      "\n",
      "Resetting Environment...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_episodes=50\n",
    "max_steps=50000\n",
    "save_every_n_iter=50\n",
    "log_every_n_iter=50\n",
    "initialize=False\n",
    "save_dir=\"deep_q_saves\"\n",
    "max_experience_buffer_len=10000\n",
    "initial_epsilon=0.5#1\n",
    "final_epsilon=0.0001\n",
    "epsilon_dec=0.00001\n",
    "train_t=1000\n",
    "discount_rate=0.9\n",
    "batch_size=120\n",
    "pickle_file_path_main_network=\"deep_q_saves/main_q_network_with_frames/model_object.pkl\"\n",
    "pickle_file_path_target_network=\"deep_q_saves/target_q_network_with_frames/model_object.pkl\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "main_Q_network=Q_Network(max_experience_buffer_len,main_q_network_params,restore_params=not initialize,pickle_file_path=pickle_file_path_main_network)\n",
    "# target_Q_network=Q_Network(0,target_q_network_params,restore_params=not initialize,pickle_file_path=pickle_file_path_target_network)\n",
    "main_Q_network.Build_model()\n",
    "# target_Q_network.Build_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if(not initialize):\n",
    "        main_Q_network.restore_model(sess,save_dir)\n",
    "\n",
    "    \n",
    "    \n",
    "#     model.test(initialize=True,env=env)\n",
    "    main_Q_network.test(sess=sess,initialize=True,env=env,sleep_time=0.1)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
