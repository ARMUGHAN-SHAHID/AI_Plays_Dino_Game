{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armughan.Shahid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "from Env import Env\n",
    "from q_network import Q_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting Environment...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\armughan.shahid\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env=Env('SpaceInvaders-v0',convert_to_grayscale=True,crop=False,valid_Y=[20,-10],valid_X=[10,-10],resize=True,resize_Y=84,resize_X=84,normalize=True,num_of_frames_per_stack=4,repeat_action=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copy_weight_ops(from_network,to_network,tau):\n",
    "    tau=tf.placeholder(tf.float32,[])\n",
    "    op_holder = []\n",
    "    for f_var,t_var in zip(from_network.model_trainable_variables,to_network.model_trainable_variables):\n",
    "        value=(f_var.value()*tau)+((1-tau)*t_var.value())\n",
    "        op_holder.append(t_var.assign(value))\n",
    "    return tau,op_holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_double_deep_Q(main_Q_network,target_Q_network,sess,episodes,steps,initial_epsilon,final_epsilon,epsilon_dec,\n",
    "                              train_t,discount_rate,batch_size,env,save_dir,save_every_n_iter,log_every_n_iter,\n",
    "                              initialize=False,set_logging=True,num_frames_to_repeat_action=4,train_main_every_n_steps=5,\n",
    "                              update_target_every_n_iters=100,tau=0.001):\n",
    "    tau_placeholder,copy_ops=make_copy_weight_ops(main_Q_network,target_Q_network,tau)\n",
    "    if initialize:\n",
    "        print (\"Initializing.....\\n\")\n",
    "        sess.run([main_Q_network.initializer])\n",
    "    sess.run([target_Q_network.initializer])\n",
    "    sess.run(copy_ops,feed_dict={tau_placeholder:1.0})\n",
    "#     else:\n",
    "        \n",
    "    if set_logging:\n",
    "        print (\"Setting up for Logging ...\\n\")\n",
    "        log_dir,set_logging=main_Q_network.create_log_directory_if_doesnt_exist(save_dir)\n",
    "    if set_logging: #creating file handlers if dir cretaed or found in above statement\n",
    "        print(\"Logging called but no code implemented\")\n",
    "#                 train_writer = tf.summary.FileWriter(os.path.join(log_dir,'train'), sess.graph)\n",
    "#                 validation_writer = tf.summary.FileWriter(os.path.join(log_dir ,'validation'))\n",
    "    print (\"Retreiveing step no...\\n\")\n",
    "    [iter_no]=sess.run([main_Q_network.step_no]) \n",
    "    epsilon=initial_epsilon\n",
    "    for episode in np.arange(episodes):\n",
    "        state=env.reset()\n",
    "        step=0\n",
    "        episode_reward=0\n",
    "        episode_loss=0\n",
    "#         previous_action=None\n",
    "        for step in np.arange(steps):\n",
    "            #choosing action \n",
    "#             if step%num_frames_to_repeat_action==0:\n",
    "            if epsilon>final_epsilon and iter_no>train_t:\n",
    "                    epsilon-=epsilon_dec\n",
    "\n",
    "#                 if  iter_no<train_t or (np.random.random(1)<epsilon):\n",
    "            if  (np.random.random(1)<epsilon):\n",
    "                action=np.random.randint(low=0,high=main_Q_network.params.num_outputs,size=1,dtype=np.int32)\n",
    "            else:\n",
    "                feed_dict={main_Q_network.X:np.expand_dims(state,axis=0),main_Q_network.lr_placeholder:main_Q_network.params.learning_rate,main_Q_network.training_mode:True}\n",
    "                [action]=sess.run([main_Q_network.max_q_value_actions],feed_dict=feed_dict)\n",
    "            action=np.squeeze(action)\n",
    "#             else:\n",
    "#                 action=previous_action\n",
    "#                 print(action)\n",
    "            next_state,reward,done,info=env.step(action)\n",
    "            episode_reward+=reward\n",
    "            main_Q_network.add_to_experience_replay(state,action,next_state,reward,done)\n",
    "#             previous_action=action\n",
    "            episode_has_finished=done\n",
    "\n",
    "            state=next_state\n",
    "\n",
    "            if (main_Q_network.experience_replay_buffer.num_items>train_t )and (step%train_main_every_n_steps==0): #perform training if there are enough experiences\n",
    "#                     print(\"buffer filled\")\n",
    "\n",
    "\n",
    "                #performing training step\n",
    "                states,actions,next_states,rewards,dones=main_Q_network.experience_replay_buffer.get_batch(batch_size=batch_size)\n",
    "\n",
    "\n",
    "                #finding vals of next states\n",
    "#                     print (next_states.shape)\n",
    "                feed_dict={target_Q_network.X:next_states,\n",
    "                           target_Q_network.lr_placeholder:target_Q_network.params.learning_rate,\n",
    "                           target_Q_network.training_mode:True}\n",
    "                #double dqn part\n",
    "                [q_vals]=sess.run([target_Q_network.logits],feed_dict=feed_dict)\n",
    "                \n",
    "                feed_dict={main_Q_network.X:next_states,\n",
    "                           main_Q_network.training_mode:True}\n",
    "                [target_actions]=sess.run([main_Q_network.max_q_value_actions],feed_dict=feed_dict)\n",
    "                target_actions=np.squeeze(target_actions)\n",
    "                max_q_vals_next_state=q_vals[np.arange(q_vals.shape[0]),target_actions]\n",
    "            \n",
    "#                 [max_q_vals_next_state]=sess.run([target_Q_network.max_q_values],feed_dict=feed_dict)\n",
    "\n",
    "                feed_dict={main_Q_network.X:states,main_Q_network.actions:actions,\n",
    "                           main_Q_network.max_q_values_next_state:max_q_vals_next_state,main_Q_network.rewards:rewards,\n",
    "                           main_Q_network.notended:((np.logical_not(dones)).astype(np.int32)),\n",
    "                           main_Q_network.discount_rate:discount_rate,main_Q_network.lr_placeholder:main_Q_network.params.learning_rate,\n",
    "                           main_Q_network.training_mode:True}\n",
    "                loss,_=sess.run([main_Q_network.loss,main_Q_network.train_op],feed_dict=feed_dict)\n",
    "                episode_loss+=loss\n",
    "                iter_no+=1\n",
    "                if (iter_no)%update_target_every_n_iters==0:\n",
    "                    print(\"updating weights of target network\\n\")\n",
    "                    sess.run(copy_ops,feed_dict={tau_placeholder:tau})\n",
    "                \n",
    "                if (iter_no)%save_every_n_iter==0:\n",
    "                    print(\"^^^^ saving model ^^^^ \\n\")\n",
    "                    main_Q_network.save_model(sess,save_dir,main_Q_network.step_no)\n",
    "                if (iter_no)%log_every_n_iter==0:\n",
    "                    print (\"Trainaing Step:\\t Iteration no={} Game Step ={} loss={} \".format(iter_no,step,loss))\n",
    "            if episode_has_finished:\n",
    "                break\n",
    "        print (\"===================>Episode {} Ended <===================\\n\".format(episode)) \n",
    "        print (\"=======>\\t Episode Length={} \\t<=======\\n\".format(step))   \n",
    "        print (\"=======>\\t Episode Reward={} \\t<=======\\n\".format(episode_reward))\n",
    "        print (\"=======>\\t Mean Episode Loss={} \\t<=======\\n\".format(episode_loss/step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_q_network_params={\n",
    "    'input_shape':[None, *env.image_shape],\n",
    "    'num_outputs':env.action_space,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':4,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':3,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':512},\n",
    "        {'layer_type':'activation_layer'}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.elu,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'main_q_network_with_frames'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_network_params={\n",
    "    'input_shape':[None, *env.image_shape],\n",
    "    'num_outputs':env.action_space,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':4,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':3,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':512},\n",
    "        {'layer_type':'activation_layer'}\n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.elu,\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'target_q_network_with_frames'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing.....\n",
      "\n",
      "Setting up for Logging ...\n",
      "\n",
      "Logging called but no code implemented\n",
      "Retreiveing step no...\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 0 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=164 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=35.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 1 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=146 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=0.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 2 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=106 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=15.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 3 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=205 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=0.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 4 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=168 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=25.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "===================>Episode 5 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=98 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=15.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.0 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=50.0 Game Step =156 loss=0.32996001839637756 \n",
      "===================>Episode 6 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=200 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=25.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.6898544894903899 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=100.0 Game Step =5 loss=0.35774868726730347 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=150.0 Game Step =55 loss=0.18389570713043213 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=200.0 Game Step =105 loss=0.17004652321338654 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=250.0 Game Step =155 loss=0.05895612761378288 \n",
      "===================>Episode 7 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=175 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=35.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2802378361565726 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=300.0 Game Step =29 loss=0.114824578166008 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=350.0 Game Step =79 loss=0.2721923589706421 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=400.0 Game Step =129 loss=0.1892480105161667 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=450.0 Game Step =179 loss=0.040747061371803284 \n",
      "===================>Episode 8 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=202 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=60.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.2542679721010056 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=500.0 Game Step =26 loss=0.03112153522670269 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=550.0 Game Step =76 loss=0.13331684470176697 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=600.0 Game Step =126 loss=0.11886949837207794 \n",
      "===================>Episode 9 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=155 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=35.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.1016405068458088 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=650.0 Game Step =20 loss=0.13714644312858582 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=700.0 Game Step =70 loss=0.06835687905550003 \n",
      "===================>Episode 10 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=118 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=5.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.17708373375994674 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=750.0 Game Step =1 loss=0.040030669420957565 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=800.0 Game Step =51 loss=0.026098676025867462 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=850.0 Game Step =101 loss=0.0354086272418499 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=900.0 Game Step =151 loss=0.09264495223760605 \n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=950.0 Game Step =201 loss=0.20921406149864197 \n",
      "===================>Episode 11 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=241 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=45.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.19192062309050462 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "updating weights of target network\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=1000.0 Game Step =9 loss=0.06794556975364685 \n"
     ]
    }
   ],
   "source": [
    "n_episodes=50\n",
    "max_steps=50000\n",
    "save_every_n_iter=50\n",
    "log_every_n_iter=50\n",
    "initialize=True#False\n",
    "save_dir=\"deep_q_saves\"\n",
    "max_experience_buffer_len=10000\n",
    "initial_epsilon=0.5#1\n",
    "final_epsilon=0.01\n",
    "epsilon_dec=0.0001\n",
    "train_t=1000\n",
    "discount_rate=0.99\n",
    "batch_size=200\n",
    "pickle_file_path_main_network=\"deep_q_saves/main_q_network_with_frames/model_object.pkl\"\n",
    "pickle_file_path_target_network=\"deep_q_saves/target_q_network_with_frames/model_object.pkl\"\n",
    "tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "main_Q_network=Q_Network(max_experience_buffer_len,main_q_network_params,restore_params=not initialize,pickle_file_path=pickle_file_path_main_network)\n",
    "target_Q_network=Q_Network(0,target_q_network_params,restore_params=False,pickle_file_path=pickle_file_path_target_network)\n",
    "main_Q_network.Build_model()\n",
    "target_Q_network.Build_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if(not initialize):\n",
    "        main_Q_network.restore_model(sess,save_dir)\n",
    "\n",
    "    \n",
    "    train_using_double_deep_Q(main_Q_network=main_Q_network,target_Q_network=target_Q_network,sess=sess,episodes=n_episodes,\n",
    "                              steps=max_steps,initial_epsilon=initial_epsilon,final_epsilon=final_epsilon,\n",
    "                              epsilon_dec=epsilon_dec,train_t=train_t,discount_rate=discount_rate,batch_size=batch_size,\n",
    "                              env=env,save_dir=save_dir,save_every_n_iter=save_every_n_iter,\n",
    "                              log_every_n_iter=log_every_n_iter,initialize=initialize,set_logging=True,\n",
    "                              num_frames_to_repeat_action=4,train_main_every_n_steps=1,update_target_every_n_iters=50,tau=0.001)\n",
    "#     model.test(initialize=True,env=env)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_episodes=50\n",
    "# max_steps=50000\n",
    "# save_every_n_iter=50\n",
    "# log_every_n_iter=50\n",
    "# initialize=False\n",
    "# save_dir=\"deep_q_saves\"\n",
    "# max_experience_buffer_len=10000\n",
    "# initial_epsilon=0.5#1\n",
    "# final_epsilon=0.0001\n",
    "# epsilon_dec=0.00001\n",
    "# train_t=1000\n",
    "# discount_rate=0.9\n",
    "# batch_size=120\n",
    "# pickle_file_path_main_network=\"deep_q_saves/main_q_network_with_frames/model_object.pkl\"\n",
    "# pickle_file_path_target_network=\"deep_q_saves/target_q_network_with_frames/model_object.pkl\"\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "# main_Q_network=Q_Network(max_experience_buffer_len,main_q_network_params,restore_params=not initialize,pickle_file_path=pickle_file_path_main_network)\n",
    "# # target_Q_network=Q_Network(0,target_q_network_params,restore_params=not initialize,pickle_file_path=pickle_file_path_target_network)\n",
    "# main_Q_network.Build_model()\n",
    "# # target_Q_network.Build_model()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "    \n",
    "#     if(not initialize):\n",
    "#         main_Q_network.restore_model(sess,save_dir)\n",
    "\n",
    "    \n",
    "    \n",
    "# #     model.test(initialize=True,env=env)\n",
    "#     main_Q_network.test(sess=sess,initialize=True,env=env,sleep_time=0.1)\n",
    "#     env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
