{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armughan.Shahid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import CNN_Model,Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(CNN_Model):\n",
    "    def __init__(self,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        CNN_Model.__init__(self,param_dict,restore_params,pickle_file_path)\n",
    "        \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(labels=targets,logits=logits)\n",
    "        return entropies\n",
    "        \n",
    "    def Build_model(self):\n",
    "        self.build_model_till_logits()\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            \n",
    "#             self.Advantage=self.form_placeholder((None,1),tf.float32)\n",
    "            self.probs_all_actions=tf.nn.softmax(self.logits)#converting to probs\n",
    "            log_likelihood_all_actions=tf.log(self.probs_all_actions)\n",
    "        \n",
    "            #which action to take\n",
    "            self.desired_action=tf.multinomial(log_likelihood_all_actions,num_samples=1)\n",
    "\n",
    "#           we will be using sparse softmax cross entropy function which will give us neg ;p likelihoods for yje selected action\n",
    "            self.neg_log_likelihood=self.form_loss(self.logits,self.desired_action[:][0])\n",
    "           #computing gradients \n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.grads_and_vars=optimizer.compute_gradients(loss=self.neg_log_likelihood,var_list=self.model_trainable_variables)\n",
    "            \n",
    "            \n",
    "#             #for easy manipulation of grads\n",
    "            self.grads=np.array([grad for grad,var_name in self.grads_and_vars])\n",
    "            \n",
    "#             #placeholder for all gradients (list of placeholder and var name tuples)\n",
    "#             print (self.grads_and_vars[0])\n",
    "            self.grad_placeholders=[self.form_placeholder(grad.get_shape(),tf.float32) for (grad,_) in self.grads_and_vars]\n",
    "            self.grads_and_vars_feed=[(self.grad_placeholders[i],self.grads_and_vars[i][1]) for i in np.arange(len(self.grads_and_vars)) ]\n",
    "            #linking placeholder and var name\n",
    "            \n",
    "#             #taking grad step based on the values of grads received \n",
    "            self.train_op=optimizer.apply_gradients(grads_and_vars=self.grads_and_vars_feed,global_step=self.step_no)\n",
    "            \n",
    "            self.initializer=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'input_shape':[None, 35, 190, 1],\n",
    "    'num_outputs':3,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':1,'num_filters':16,'padding':'valid'},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':1,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':256},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'dropout_layer','dropout_probability':0.5},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':100},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'dropout_layer','dropout_probability':0.5}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.relu,\n",
    "    'loss_fn':tf.nn.sparse_softmax_cross_entropy_with_logits, #carefull\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'neural_network_bn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'neural_network_bn_1/gradients/neural_network_bn/conv2d/Conv2D_grad/tuple/control_dependency_1:0' shape=(8, 8, 1, 16) dtype=float32>, <tf.Variable 'neural_network_bn/conv2d/kernel:0' shape=(8, 8, 1, 16) dtype=float32_ref>)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model=Policy_Network(params)\n",
    "model.Build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
