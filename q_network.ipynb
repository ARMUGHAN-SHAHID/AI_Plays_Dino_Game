{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armughan.Shahid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import CNN_Model,Params\n",
    "from Env import Env\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_replay_buffer:\n",
    "#     def __init__(self,size,dtypes):\n",
    "#         self.column_names=['state','action','next_state','reward','done']\n",
    "#         self.buffer={self.column_names[i]:np.empty(size,dtype=dtypes[i]) for i in np.arange(len(self.column_names)) }\n",
    "#         self.num_items=0\n",
    "#         self.capacity=size\n",
    "    def __init__(self,buffer_len,sample):\n",
    "        self.column_names=['state','action','next_state','reward','done']\n",
    "        self.buffer={col_name:np.empty(shape=[buffer_len,*np.array(item).shape],dtype=np.array(item).dtype) for col_name,item in zip(self.column_names,sample) }\n",
    "        self.num_items=0\n",
    "        self.capacity=buffer_len\n",
    "        self.add_ind=0\n",
    "    def add_experience(self,state,action,next_state,reward,done):\n",
    "        ind=self.add_ind\n",
    "        self.add_ind=(self.add_ind+1)%self.capacity\n",
    "#         ind=self.num_items\n",
    "        if self.num_items<self.capacity:\n",
    "            self.num_items+=1\n",
    "#         else:\n",
    "#             ind=np.random.randint(low=0,high=self.capacity,size=1,dtype=np.int32)\n",
    "            \n",
    "        self.buffer['state'][ind]=state\n",
    "        self.buffer['action'][ind]=action\n",
    "        self.buffer['next_state'][ind]=next_state\n",
    "        self.buffer['reward'][ind]=reward\n",
    "        self.buffer['done'][ind]=done\n",
    "           \n",
    "    \n",
    "    def get_batch(self,batch_size):\n",
    "        inds=np.random.randint(low=0,high=self.num_items,size=batch_size,dtype=np.int32)\n",
    "        return self.buffer['state'][inds],self.buffer['action'][inds],self.buffer['next_state'][inds],self.buffer['reward'][inds],self.buffer['done'][inds]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(CNN_Model):\n",
    "    def __init__(self,max_experience_buffer_len=120,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        CNN_Model.__init__(self,param_dict,restore_params,pickle_file_path)\n",
    "        self.max_experience_buffer_len=max_experience_buffer_len\n",
    "        \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(labels=targets,logits=logits)\n",
    "        return entropies\n",
    "        \n",
    "    def Build_model(self):\n",
    "        self.build_model_till_logits()\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            #logits are q values]\n",
    "            self.max_q_value_actions=tf.squeeze(tf.argmax(self.logits,axis=1)) #value which has the highest q value\n",
    "            self.max_q_values=tf.reduce_max(self.logits,axis=1)\n",
    "            \n",
    "            #placeholder for action at current timestep\n",
    "            self.actions=self.form_placeholder((None),tf.int32)\n",
    "            one_hot_actions=tf.one_hot(indices=self.actions,depth=self.params.num_outputs)\n",
    "            q_vals=tf.reduce_sum(self.logits*one_hot_actions,axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #placeholder for max next state q values,rewards and discount rate\n",
    "            self.max_q_values_next_state=self.form_placeholder((None),tf.float32)\n",
    "            self.rewards=self.form_placeholder((None),tf.float32)\n",
    "            self.notended=self.form_placeholder((None),tf.float32)\n",
    "            self.discount_rate=self.form_placeholder([],tf.float32)\n",
    "            \n",
    "            self.loss=tf.reduce_mean(tf.square((self.rewards+(self.discount_rate*(self.max_q_values_next_state*self.notended)))-q_vals))\n",
    "#             self.qvalues_next=self.max_q_values_next_state*self.notended\n",
    "#             self.discounted_qvalues_next=self.discount_rate*self.qvalues_next\n",
    "#             self.targets=self.rewards+self.discounted_qvalues_next\n",
    "#             self.diff=self.targets-q_vals\n",
    "#             self.squared_diff=tf.square(self.diff)\n",
    "#             self.loss=tf.reduce_mean(self.squared_diff)\n",
    "        \n",
    "            #computing gradients \n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.grads_and_vars=optimizer.compute_gradients(loss=self.loss,var_list=self.model_trainable_variables)\n",
    "            \n",
    "            self.train_op=optimizer.apply_gradients(grads_and_vars=self.grads_and_vars,global_step=self.step_no)\n",
    "            self.model_variables=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.params.name_scope)\n",
    "            self.saver=tf.train.Saver(var_list=self.model_variables)\n",
    "\n",
    "            self.initializer=tf.global_variables_initializer()\n",
    "    \n",
    "    def add_to_experience_replay(self,state,action,next_state,reward,done):\n",
    "        \n",
    "        if not hasattr(self,\"experience_replay_buffer\"):\n",
    "            sample=[state,action,next_state,reward,done]\n",
    "            self.experience_replay_buffer=experience_replay_buffer(buffer_len=self.max_experience_buffer_len,sample=sample)\n",
    "        self.experience_replay_buffer.add_experience(state,action,next_state,reward,done)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self,sess,episodes,steps,initial_epsilon,final_epsilon,epsilon_dec,train_t,discount_rate,batch_size,env,save_dir,save_every_n_iter,log_every_n_iter,initialize=False,set_logging=True,num_frames_to_repeat_action=4):\n",
    "        if initialize:\n",
    "            print (\"Initializing.....\\n\")\n",
    "            sess.run([self.initializer])\n",
    "        if set_logging:\n",
    "            print (\"Setting up for Logging ...\\n\")\n",
    "            log_dir,set_logging=self.create_log_directory_if_doesnt_exist(save_dir)\n",
    "        if set_logging: #creating file handlers if dir cretaed or found in above statement\n",
    "            print(\"Logging called but no code implemented\")\n",
    "#                 train_writer = tf.summary.FileWriter(os.path.join(log_dir,'train'), sess.graph)\n",
    "#                 validation_writer = tf.summary.FileWriter(os.path.join(log_dir ,'validation'))\n",
    "        print (\"Retreiveing step no...\\n\")\n",
    "        [iter_no]=sess.run([self.step_no]) \n",
    "        epsilon=initial_epsilon\n",
    "        for episode in np.arange(episodes):\n",
    "            state=env.reset()\n",
    "            step=0\n",
    "            episode_reward=0\n",
    "            episode_loss=0\n",
    "            previous_action=None\n",
    "            for step in np.arange(steps):\n",
    "                #choosing action \n",
    "#                 action=-1\n",
    "                if step%num_frames_to_repeat_action==0:\n",
    "                    if epsilon>final_epsilon and iter_no>train_t:\n",
    "                            epsilon-=epsilon_dec\n",
    "\n",
    "    #                 if  iter_no<train_t or (np.random.random(1)<epsilon):\n",
    "                    if  (np.random.random(1)<epsilon):\n",
    "                        action=np.random.randint(low=0,high=self.params.num_outputs,size=1,dtype=np.int32)\n",
    "                    else:\n",
    "                        feed_dict={self.X:np.expand_dims(state,axis=0),self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                        [action]=sess.run([self.max_q_value_actions],feed_dict=feed_dict)\n",
    "                    action=np.squeeze(action)\n",
    "                else:\n",
    "                    action=previous_action\n",
    "#                 print(action)\n",
    "                next_state,reward,done,info=env.step(action)\n",
    "                episode_reward+=reward\n",
    "                self.add_to_experience_replay(state,action,next_state,reward,done)\n",
    "                previous_action=action\n",
    "                episode_has_finished=done\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                if self.experience_replay_buffer.num_items>train_t: #perform training if there are enough experiences\n",
    "#                     print(\"buffer filled\")\n",
    "                    \n",
    "                    \n",
    "                    #performing training step\n",
    "                    states,actions,next_states,rewards,dones=self.experience_replay_buffer.get_batch(batch_size=batch_size)\n",
    "                    \n",
    "\n",
    "                    #finding vals of next states\n",
    "#                     print (next_states.shape)\n",
    "                    feed_dict={self.X:next_states,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    [max_q_vals_next_state]=sess.run([self.max_q_values],feed_dict=feed_dict)\n",
    "#                     max_q_vals_next_state[nan_inds]=0\n",
    "\n",
    "                    #calculating loss and running train op\n",
    "#                     print(\"shapes\\n\")\n",
    "#                     print(states.shape)\n",
    "#                     print(actions.shape)\n",
    "#                     print(max_q_vals_next_state.shape)\n",
    "#                     print(rewards.shape)\n",
    "#                     print(dones.shape)\n",
    "#                     print(discount_rate)\n",
    "                    \n",
    "#                     print(\"li\\n\")\n",
    "#                     print(states)\n",
    "#                     print(actions)\n",
    "#                     print(max_q_vals_next_state)\n",
    "#                     print(rewards)\n",
    "#                     print(dones)\n",
    "#                     print(discount_rate)\n",
    "#                     self.qvalues_next=self.max_q_values_next_state*self.notended\n",
    "#             self.discounted_qvalues_next=self.discount_rate*qvalues_next\n",
    "#             self.targets=self.rewards+discounted_qvalues_next\n",
    "#             self.diff=targets-q_vals\n",
    "#             self.squared_diff=tf.square(diff)\n",
    "#             self.loss=tf.reduce_mean(squared_diff)\n",
    "                    feed_dict={self.X:states,self.actions:actions,self.max_q_values_next_state:max_q_vals_next_state,self.rewards:rewards,self.notended:((np.logical_not(dones)).astype(np.int32)),self.discount_rate:discount_rate,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    loss,_=sess.run([self.loss,self.train_op],feed_dict=feed_dict)\n",
    "                    episode_loss+=loss\n",
    "                    iter_no+=1\n",
    "                    if (iter_no)%save_every_n_iter==0:\n",
    "                        print(\"^^^^ saving model ^^^^ \\n\")\n",
    "                        self.save_model(sess,save_dir,self.step_no)\n",
    "                    if (iter_no)%log_every_n_iter==0:\n",
    "                        print (\"Trainaing Step:\\t Iteration no={} Game Step ={} loss={} \".format(iter_no,step,loss))\n",
    "                if episode_has_finished:\n",
    "                    break\n",
    "            print (\"===================>Episode {} Ended <===================\\n\".format(episode)) \n",
    "            print (\"=======>\\t Episode Length={} \\t<=======\\n\".format(step))   \n",
    "            print (\"=======>\\t Episode Reward={} \\t<=======\\n\".format(episode_reward))\n",
    "            print (\"=======>\\t Mean Episode Loss={} \\t<=======\\n\".format(episode_loss/step))\n",
    "            \n",
    "    def test(self,initialize,env,sleep_time=0.5):\n",
    "        if initialize:\n",
    "            print (\"Initializing.....\\n\")\n",
    "            sess.run([self.initializer])\n",
    "            \n",
    "        state=env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            feed_dict={self.X:np.expand_dims(state,axis=0),self.lr_placeholder:self.params.learning_rate,self.training_mode:False}\n",
    "            [action]=sess.run([self.max_q_value_actions],feed_dict=feed_dict)\n",
    "            action=np.squeeze(action)\n",
    "            state,reward,done,info=env.step(action)\n",
    "            env.render()\n",
    "            sleep(sleep_time)\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\armughan.shahid\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting Environment...\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "env=Env('SpaceInvaders-v0',convert_to_grayscale=True,crop=True,valid_Y=[20,-10],valid_X=[10,-10],resize=True,resize_Y=90,resize_X=70,normalize=True,num_of_frames_per_stack=4)\n",
    "params={\n",
    "    'input_shape':[None, *env.image_shape],\n",
    "    'num_outputs':env.action_space,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':4,'num_filters':32,'padding':'valid'},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':3,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':512},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'}\n",
    "#         {'layer_type':'dropout_layer','dropout_probability':0.2},\n",
    "#         {'layer_type':'fc_layer','num_hidden_units':50},\n",
    "# #         {'layer_type':'batch_normalization_layer'},\n",
    "#         {'layer_type':'activation_layer'}\n",
    "#         {'layer_type':'dropout_layer','dropout_probability':0.2}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.elu,\n",
    "#     'loss_fn':tf.nn.sparse_softmax_cross_entropy_with_logits, #carefull\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'q_network_with_frames'\n",
    "}\n",
    "print (params['num_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring path:D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\q_network_with_frames\n",
      "INFO:tensorflow:Restoring parameters from D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\q_network_with_frames\\model_weights.ckpt-81800\n",
      "Setting up for Logging ...\n",
      "\n",
      "Logging called but no code implemented\n",
      "Retreiveing step no...\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=81850.0 Game Step =1049 loss=5.992928981781006 \n",
      "===================>Episode 0 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1051 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=270.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=0.3245964736512 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=81900.0 Game Step =47 loss=1.0200822353363037 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=81950.0 Game Step =97 loss=2.583500623703003 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82000.0 Game Step =147 loss=1.9424474239349365 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82050.0 Game Step =197 loss=4.226060390472412 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82100.0 Game Step =247 loss=2.010183334350586 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82150.0 Game Step =297 loss=3.108027219772339 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82200.0 Game Step =347 loss=2.9081194400787354 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82250.0 Game Step =397 loss=5.059940814971924 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82300.0 Game Step =447 loss=3.041618585586548 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82350.0 Game Step =497 loss=3.939546823501587 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82400.0 Game Step =547 loss=1.342195987701416 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82450.0 Game Step =597 loss=1.9619500637054443 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82500.0 Game Step =647 loss=1.4061510562896729 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82550.0 Game Step =697 loss=2.619825839996338 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82600.0 Game Step =747 loss=1.8209502696990967 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82650.0 Game Step =797 loss=2.2042698860168457 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82700.0 Game Step =847 loss=1.2568386793136597 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82750.0 Game Step =897 loss=1.2732272148132324 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82800.0 Game Step =947 loss=1.026594638824463 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82850.0 Game Step =997 loss=2.043290376663208 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82900.0 Game Step =1047 loss=1.3773781061172485 \n",
      "===================>Episode 1 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1059 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=325.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=2.549797328718436 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=82950.0 Game Step =37 loss=2.130746603012085 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83000.0 Game Step =87 loss=2.448085069656372 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83050.0 Game Step =137 loss=1.550487756729126 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83100.0 Game Step =187 loss=1.4493813514709473 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83150.0 Game Step =237 loss=2.2816431522369385 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83200.0 Game Step =287 loss=2.656174659729004 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83250.0 Game Step =337 loss=2.2519781589508057 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83300.0 Game Step =387 loss=2.4402551651000977 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83350.0 Game Step =437 loss=1.0229182243347168 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83400.0 Game Step =487 loss=0.8114389181137085 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83450.0 Game Step =537 loss=2.447089672088623 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83500.0 Game Step =587 loss=0.6054771542549133 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83550.0 Game Step =637 loss=2.741236925125122 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83600.0 Game Step =687 loss=2.126561403274536 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83650.0 Game Step =737 loss=1.2929922342300415 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83700.0 Game Step =787 loss=4.78870964050293 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83750.0 Game Step =837 loss=2.684779644012451 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83800.0 Game Step =887 loss=4.237417697906494 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83850.0 Game Step =937 loss=6.178246021270752 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83900.0 Game Step =987 loss=7.41603422164917 \n",
      "===================>Episode 2 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=989 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=435.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=6.372572845648466 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=83950.0 Game Step =47 loss=6.412917137145996 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84000.0 Game Step =97 loss=2.9413702487945557 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84050.0 Game Step =147 loss=14.910909652709961 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84100.0 Game Step =197 loss=15.424725532531738 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84150.0 Game Step =247 loss=5.745296955108643 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84200.0 Game Step =297 loss=4.171979904174805 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84250.0 Game Step =347 loss=12.296142578125 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84300.0 Game Step =397 loss=4.621865749359131 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84350.0 Game Step =447 loss=10.689766883850098 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84400.0 Game Step =497 loss=4.047885417938232 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84450.0 Game Step =547 loss=7.219755172729492 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84500.0 Game Step =597 loss=6.5205769538879395 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84550.0 Game Step =647 loss=5.845455646514893 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84600.0 Game Step =697 loss=5.3833160400390625 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84650.0 Game Step =747 loss=98.072998046875 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84700.0 Game Step =797 loss=5.90872049331665 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84750.0 Game Step =847 loss=24.045793533325195 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84800.0 Game Step =897 loss=32.96765899658203 \n",
      "===================>Episode 3 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=946 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=210.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=17.00225260078781 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84850.0 Game Step =0 loss=15.674304008483887 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84900.0 Game Step =50 loss=7.532547950744629 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=84950.0 Game Step =100 loss=12.248368263244629 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85000.0 Game Step =150 loss=11.440851211547852 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85050.0 Game Step =200 loss=14.790060997009277 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85100.0 Game Step =250 loss=6.533523082733154 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85150.0 Game Step =300 loss=13.796799659729004 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85200.0 Game Step =350 loss=12.214299201965332 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85250.0 Game Step =400 loss=7.496454238891602 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85300.0 Game Step =450 loss=6.594360828399658 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85350.0 Game Step =500 loss=3.1520955562591553 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85400.0 Game Step =550 loss=3.5137531757354736 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85450.0 Game Step =600 loss=5.39811372756958 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85500.0 Game Step =650 loss=4.614497184753418 \n",
      "===================>Episode 4 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=654 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=60.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=9.695361201187158 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85550.0 Game Step =45 loss=3.140728712081909 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85600.0 Game Step =95 loss=2.369758367538452 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85650.0 Game Step =145 loss=6.662763595581055 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85700.0 Game Step =195 loss=2.478902578353882 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85750.0 Game Step =245 loss=23.325780868530273 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85800.0 Game Step =295 loss=2.3870997428894043 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85850.0 Game Step =345 loss=2.260279893875122 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85900.0 Game Step =395 loss=2.357161521911621 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=85950.0 Game Step =445 loss=6.14884090423584 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86000.0 Game Step =495 loss=3.5940401554107666 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86050.0 Game Step =545 loss=2.107578754425049 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86100.0 Game Step =595 loss=2.87062668800354 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86150.0 Game Step =645 loss=6.593048572540283 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86200.0 Game Step =695 loss=2.737157106399536 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86250.0 Game Step =745 loss=6.103029727935791 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86300.0 Game Step =795 loss=2.1832587718963623 \n",
      "===================>Episode 5 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=829 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=230.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=5.943126418334583 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86350.0 Game Step =15 loss=2.3907296657562256 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86400.0 Game Step =65 loss=1.2101789712905884 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86450.0 Game Step =115 loss=1.89017915725708 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86500.0 Game Step =165 loss=5.242524147033691 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86550.0 Game Step =215 loss=1.5622740983963013 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86600.0 Game Step =265 loss=4.654103755950928 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86650.0 Game Step =315 loss=4.526730060577393 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86700.0 Game Step =365 loss=1.9319044351577759 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86750.0 Game Step =415 loss=1.9343618154525757 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86800.0 Game Step =465 loss=5.708465099334717 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86850.0 Game Step =515 loss=1.5964595079421997 \n",
      "===================>Episode 6 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=555 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=50.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=4.987205115417103 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86900.0 Game Step =9 loss=6.19596004486084 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=86950.0 Game Step =59 loss=2.764404773712158 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87000.0 Game Step =109 loss=1.7117441892623901 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87050.0 Game Step =159 loss=3.9268758296966553 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87100.0 Game Step =209 loss=3.081726551055908 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87150.0 Game Step =259 loss=3.3568098545074463 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87200.0 Game Step =309 loss=2.52347469329834 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87250.0 Game Step =359 loss=5.122432231903076 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87300.0 Game Step =409 loss=3.039719820022583 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87350.0 Game Step =459 loss=4.945017337799072 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87400.0 Game Step =509 loss=3.6788341999053955 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87450.0 Game Step =559 loss=2.02170991897583 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87500.0 Game Step =609 loss=1.8953449726104736 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87550.0 Game Step =659 loss=3.749702215194702 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87600.0 Game Step =709 loss=3.4752259254455566 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87650.0 Game Step =759 loss=3.0568325519561768 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87700.0 Game Step =809 loss=1.8381632566452026 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87750.0 Game Step =859 loss=6.213696479797363 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87800.0 Game Step =909 loss=3.565138339996338 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87850.0 Game Step =959 loss=3.06899356842041 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87900.0 Game Step =1009 loss=1.7762939929962158 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=87950.0 Game Step =1059 loss=5.919586181640625 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88000.0 Game Step =1109 loss=2.29667067527771 \n",
      "===================>Episode 7 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1125 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=310.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=4.615137857596079 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88050.0 Game Step =33 loss=1.9506930112838745 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88100.0 Game Step =83 loss=1.9759516716003418 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88150.0 Game Step =133 loss=2.0950865745544434 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88200.0 Game Step =183 loss=1.4598225355148315 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88250.0 Game Step =233 loss=2.205487012863159 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88300.0 Game Step =283 loss=2.4214181900024414 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88350.0 Game Step =333 loss=2.6677169799804688 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88400.0 Game Step =383 loss=2.2822182178497314 \n",
      "===================>Episode 8 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=385 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=10.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=3.338350852433737 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88450.0 Game Step =47 loss=4.675971031188965 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88500.0 Game Step =97 loss=2.633229970932007 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88550.0 Game Step =147 loss=2.9338879585266113 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88600.0 Game Step =197 loss=2.853278875350952 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88650.0 Game Step =247 loss=5.125741958618164 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88700.0 Game Step =297 loss=1.78154718875885 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88750.0 Game Step =347 loss=1.1617988348007202 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88800.0 Game Step =397 loss=5.506714344024658 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88850.0 Game Step =447 loss=2.4468798637390137 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88900.0 Game Step =497 loss=2.317195415496826 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=88950.0 Game Step =547 loss=4.671149730682373 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89000.0 Game Step =597 loss=1.4049108028411865 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89050.0 Game Step =647 loss=2.3621108531951904 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89100.0 Game Step =697 loss=2.757065773010254 \n",
      "===================>Episode 9 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=712 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=130.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=2.6617948695849836 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89150.0 Game Step =34 loss=1.4056401252746582 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89200.0 Game Step =84 loss=1.6140105724334717 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89250.0 Game Step =134 loss=2.049210548400879 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89300.0 Game Step =184 loss=1.1125237941741943 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89350.0 Game Step =234 loss=1.4993466138839722 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89400.0 Game Step =284 loss=1.4193280935287476 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89450.0 Game Step =334 loss=1.552783727645874 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89500.0 Game Step =384 loss=2.3977386951446533 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89550.0 Game Step =434 loss=1.6953283548355103 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89600.0 Game Step =484 loss=3.6327269077301025 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89650.0 Game Step =534 loss=1.623072862625122 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89700.0 Game Step =584 loss=1.1089215278625488 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89750.0 Game Step =634 loss=1.602558970451355 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89800.0 Game Step =684 loss=2.1850576400756836 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89850.0 Game Step =734 loss=2.385775089263916 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89900.0 Game Step =784 loss=1.1955822706222534 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=89950.0 Game Step =834 loss=1.3235172033309937 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90000.0 Game Step =884 loss=0.8396399021148682 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90050.0 Game Step =934 loss=2.0802700519561768 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90100.0 Game Step =984 loss=2.1240925788879395 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90150.0 Game Step =1034 loss=1.8369522094726562 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90200.0 Game Step =1084 loss=6.330958843231201 \n",
      "===================>Episode 10 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1119 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=320.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=2.4116014351633854 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90250.0 Game Step =14 loss=7.235411643981934 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90300.0 Game Step =64 loss=1.0890114307403564 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90350.0 Game Step =114 loss=1.506052851676941 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90400.0 Game Step =164 loss=2.845259189605713 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90450.0 Game Step =214 loss=1.2589586973190308 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90500.0 Game Step =264 loss=1.0227344036102295 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90550.0 Game Step =314 loss=2.0736660957336426 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90600.0 Game Step =364 loss=1.2396572828292847 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90650.0 Game Step =414 loss=1.5978039503097534 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90700.0 Game Step =464 loss=1.4653993844985962 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90750.0 Game Step =514 loss=2.216644287109375 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90800.0 Game Step =564 loss=2.908857822418213 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90850.0 Game Step =614 loss=2.141407012939453 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90900.0 Game Step =664 loss=4.453568458557129 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=90950.0 Game Step =714 loss=3.0925145149230957 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91000.0 Game Step =764 loss=2.9811437129974365 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91050.0 Game Step =814 loss=1.7137922048568726 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91100.0 Game Step =864 loss=1.8812354803085327 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91150.0 Game Step =914 loss=1.5731418132781982 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91200.0 Game Step =964 loss=1.7762972116470337 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91250.0 Game Step =1014 loss=1.8587415218353271 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91300.0 Game Step =1064 loss=1.8702584505081177 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91350.0 Game Step =1114 loss=1.9590939283370972 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91400.0 Game Step =1164 loss=2.8958141803741455 \n",
      "===================>Episode 11 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1203 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=425.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=4.444389459745941 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91450.0 Game Step =10 loss=2.1777760982513428 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91500.0 Game Step =60 loss=1.5336976051330566 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91550.0 Game Step =110 loss=7.2380523681640625 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91600.0 Game Step =160 loss=3.3376574516296387 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91650.0 Game Step =210 loss=2.0578386783599854 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91700.0 Game Step =260 loss=2.1854450702667236 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91750.0 Game Step =310 loss=3.4493319988250732 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91800.0 Game Step =360 loss=2.0583860874176025 \n",
      "===================>Episode 12 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=393 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=45.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=6.940109856711089 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91850.0 Game Step =16 loss=1.1627649068832397 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91900.0 Game Step =66 loss=2.2904112339019775 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=91950.0 Game Step =116 loss=4.552114486694336 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92000.0 Game Step =166 loss=3.078989267349243 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92050.0 Game Step =216 loss=1.1985797882080078 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92100.0 Game Step =266 loss=1.4287054538726807 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92150.0 Game Step =316 loss=1.633638620376587 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92200.0 Game Step =366 loss=1.4704707860946655 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92250.0 Game Step =416 loss=4.629181861877441 \n",
      "^^^^ saving model ^^^^ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainaing Step:\t Iteration no=92300.0 Game Step =466 loss=3.0655643939971924 \n",
      "===================>Episode 13 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=507 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=10.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=5.576646208763123 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92350.0 Game Step =8 loss=4.5837297439575195 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92400.0 Game Step =58 loss=5.183407306671143 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92450.0 Game Step =108 loss=2.6208388805389404 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92500.0 Game Step =158 loss=4.46624755859375 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92550.0 Game Step =208 loss=4.587515354156494 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92600.0 Game Step =258 loss=4.691145896911621 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92650.0 Game Step =308 loss=11.196797370910645 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92700.0 Game Step =358 loss=7.56419563293457 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92750.0 Game Step =408 loss=5.954381942749023 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92800.0 Game Step =458 loss=3.7012226581573486 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92850.0 Game Step =508 loss=3.860736846923828 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92900.0 Game Step =558 loss=5.848940849304199 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=92950.0 Game Step =608 loss=2.1744384765625 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93000.0 Game Step =658 loss=3.051262140274048 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93050.0 Game Step =708 loss=3.3573827743530273 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93100.0 Game Step =758 loss=1.6242163181304932 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93150.0 Game Step =808 loss=4.3878068923950195 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93200.0 Game Step =858 loss=10.577581405639648 \n",
      "===================>Episode 14 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=907 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=155.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=8.943574012048716 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93250.0 Game Step =0 loss=2.2416865825653076 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93300.0 Game Step =50 loss=2.1405346393585205 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93350.0 Game Step =100 loss=4.984583854675293 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93400.0 Game Step =150 loss=2.058641195297241 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93450.0 Game Step =200 loss=0.906223714351654 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93500.0 Game Step =250 loss=6.116479873657227 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93550.0 Game Step =300 loss=2.759671449661255 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93600.0 Game Step =350 loss=3.1681251525878906 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93650.0 Game Step =400 loss=1.60251784324646 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93700.0 Game Step =450 loss=1.8161760568618774 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93750.0 Game Step =500 loss=2.834594964981079 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93800.0 Game Step =550 loss=2.6686854362487793 \n",
      "===================>Episode 15 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=550 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=45.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=5.056358085979115 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93850.0 Game Step =49 loss=1.5608885288238525 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93900.0 Game Step =99 loss=1.4368634223937988 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=93950.0 Game Step =149 loss=2.212771415710449 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94000.0 Game Step =199 loss=3.0402140617370605 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94050.0 Game Step =249 loss=2.1490626335144043 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94100.0 Game Step =299 loss=2.1444716453552246 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94150.0 Game Step =349 loss=6.7307610511779785 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94200.0 Game Step =399 loss=1.4309765100479126 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94250.0 Game Step =449 loss=7.195314884185791 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94300.0 Game Step =499 loss=3.183718204498291 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94350.0 Game Step =549 loss=3.78365159034729 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94400.0 Game Step =599 loss=4.189638137817383 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94450.0 Game Step =649 loss=4.298984050750732 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94500.0 Game Step =699 loss=5.990968227386475 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94550.0 Game Step =749 loss=11.996339797973633 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94600.0 Game Step =799 loss=1.7461055517196655 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94650.0 Game Step =849 loss=2.39593768119812 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94700.0 Game Step =899 loss=3.3233935832977295 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94750.0 Game Step =949 loss=3.471669912338257 \n",
      "===================>Episode 16 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=977 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=250.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=7.095607000888309 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94800.0 Game Step =21 loss=3.9021482467651367 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94850.0 Game Step =71 loss=1.056020736694336 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94900.0 Game Step =121 loss=1.5645238161087036 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=94950.0 Game Step =171 loss=1.83347487449646 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95000.0 Game Step =221 loss=4.6037445068359375 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95050.0 Game Step =271 loss=2.889725685119629 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95100.0 Game Step =321 loss=3.3929269313812256 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95150.0 Game Step =371 loss=9.615740776062012 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95200.0 Game Step =421 loss=4.245201587677002 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95250.0 Game Step =471 loss=9.415376663208008 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95300.0 Game Step =521 loss=2.7199249267578125 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95350.0 Game Step =571 loss=7.7266035079956055 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95400.0 Game Step =621 loss=2.414379596710205 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95450.0 Game Step =671 loss=4.735326290130615 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95500.0 Game Step =721 loss=5.522564888000488 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95550.0 Game Step =771 loss=3.446978807449341 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95600.0 Game Step =821 loss=2.5145556926727295 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95650.0 Game Step =871 loss=4.971159934997559 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95700.0 Game Step =921 loss=13.12942123413086 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95750.0 Game Step =971 loss=4.1283650398254395 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95800.0 Game Step =1021 loss=3.223170757293701 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95850.0 Game Step =1071 loss=3.5537562370300293 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================>Episode 17 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=1082 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=275.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=6.671104383171156 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95900.0 Game Step =38 loss=6.548985958099365 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=95950.0 Game Step =88 loss=3.83906626701355 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96000.0 Game Step =138 loss=7.495528697967529 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96050.0 Game Step =188 loss=5.22810697555542 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96100.0 Game Step =238 loss=3.106646776199341 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96150.0 Game Step =288 loss=5.715848922729492 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96200.0 Game Step =338 loss=4.385396480560303 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96250.0 Game Step =388 loss=6.576199054718018 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96300.0 Game Step =438 loss=3.911311388015747 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96350.0 Game Step =488 loss=2.969533681869507 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96400.0 Game Step =538 loss=2.7082972526550293 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96450.0 Game Step =588 loss=3.0896315574645996 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96500.0 Game Step =638 loss=5.383072853088379 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96550.0 Game Step =688 loss=2.397965669631958 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96600.0 Game Step =738 loss=4.620866298675537 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96650.0 Game Step =788 loss=9.216748237609863 \n",
      "===================>Episode 18 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=821 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=205.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=6.27631086379689 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96700.0 Game Step =16 loss=5.5905561447143555 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96750.0 Game Step =66 loss=2.5070953369140625 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96800.0 Game Step =116 loss=3.326446533203125 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96850.0 Game Step =166 loss=2.1187682151794434 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96900.0 Game Step =216 loss=2.0919554233551025 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=96950.0 Game Step =266 loss=2.102346658706665 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97000.0 Game Step =316 loss=4.804216384887695 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97050.0 Game Step =366 loss=2.0310604572296143 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97100.0 Game Step =416 loss=3.0545361042022705 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97150.0 Game Step =466 loss=5.284862995147705 \n",
      "===================>Episode 19 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=482 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=80.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=5.220869654689093 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97200.0 Game Step =33 loss=4.87089729309082 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97250.0 Game Step =83 loss=124.85619354248047 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97300.0 Game Step =133 loss=6.798590660095215 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97350.0 Game Step =183 loss=8.215666770935059 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97400.0 Game Step =233 loss=4.090918064117432 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97450.0 Game Step =283 loss=3.1916120052337646 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97500.0 Game Step =333 loss=5.621504783630371 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97550.0 Game Step =383 loss=2.092550277709961 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97600.0 Game Step =433 loss=6.158475399017334 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97650.0 Game Step =483 loss=3.838968515396118 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97700.0 Game Step =533 loss=2.8109524250030518 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97750.0 Game Step =583 loss=3.616858959197998 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97800.0 Game Step =633 loss=2.6275601387023926 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97850.0 Game Step =683 loss=2.3898980617523193 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97900.0 Game Step =733 loss=1.803364634513855 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=97950.0 Game Step =783 loss=5.000313758850098 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98000.0 Game Step =833 loss=3.0257530212402344 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98050.0 Game Step =883 loss=3.8693203926086426 \n",
      "===================>Episode 20 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=924 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=205.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=4.658553585345611 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98100.0 Game Step =8 loss=1.1835370063781738 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98150.0 Game Step =58 loss=5.431891918182373 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98200.0 Game Step =108 loss=1.399169921875 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98250.0 Game Step =158 loss=2.414065361022949 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98300.0 Game Step =208 loss=7.73282527923584 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98350.0 Game Step =258 loss=5.225633144378662 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98400.0 Game Step =308 loss=1.588271975517273 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98450.0 Game Step =358 loss=2.647110939025879 \n",
      "===================>Episode 21 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=358 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=70.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=2.687350088158133 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98500.0 Game Step =49 loss=2.1431703567504883 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98550.0 Game Step =99 loss=1.1605817079544067 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98600.0 Game Step =149 loss=2.297985553741455 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98650.0 Game Step =199 loss=2.1969027519226074 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98700.0 Game Step =249 loss=2.153571128845215 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98750.0 Game Step =299 loss=2.8900411128997803 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98800.0 Game Step =349 loss=1.4828184843063354 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98850.0 Game Step =399 loss=11.4038667678833 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98900.0 Game Step =449 loss=1.9359031915664673 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=98950.0 Game Step =499 loss=0.9215863347053528 \n",
      "===================>Episode 22 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=509 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=80.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=2.576603242831052 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99000.0 Game Step =39 loss=1.2919000387191772 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99050.0 Game Step =89 loss=1.7062894105911255 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99100.0 Game Step =139 loss=2.575969934463501 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99150.0 Game Step =189 loss=3.6360435485839844 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99200.0 Game Step =239 loss=5.165910720825195 \n",
      "^^^^ saving model ^^^^ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainaing Step:\t Iteration no=99250.0 Game Step =289 loss=1.8190727233886719 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99300.0 Game Step =339 loss=1.954352855682373 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99350.0 Game Step =389 loss=5.552530288696289 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99400.0 Game Step =439 loss=4.9918599128723145 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99450.0 Game Step =489 loss=2.945768117904663 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99500.0 Game Step =539 loss=3.7758219242095947 \n",
      "===================>Episode 23 Ended <===================\n",
      "\n",
      "=======>\t Episode Length=582 \t<=======\n",
      "\n",
      "=======>\t Episode Reward=135.0 \t<=======\n",
      "\n",
      "=======>\t Mean Episode Loss=2.853358414164933 \t<=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99550.0 Game Step =6 loss=1.214494228363037 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99600.0 Game Step =56 loss=1.3414454460144043 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99650.0 Game Step =106 loss=2.583414316177368 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99700.0 Game Step =156 loss=1.429558277130127 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99750.0 Game Step =206 loss=5.236446857452393 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99800.0 Game Step =256 loss=1.9002461433410645 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99850.0 Game Step =306 loss=4.871475696563721 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99900.0 Game Step =356 loss=1.812822937965393 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=99950.0 Game Step =406 loss=2.2699339389801025 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=100000.0 Game Step =456 loss=0.9503476023674011 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=100050.0 Game Step =506 loss=1.9035296440124512 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2ad5f850e9bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_epsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon_dec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon_dec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset_logging\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m#     model.test(initialize=True,env=env)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8ef3de33fb9e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, episodes, steps, initial_epsilon, final_epsilon, epsilon_dec, train_t, discount_rate, batch_size, env, save_dir, save_every_n_iter, log_every_n_iter, initialize, set_logging, num_frames_to_repeat_action)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;31m#             self.loss=tf.reduce_mean(squared_diff)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_q_values_next_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_q_vals_next_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotended\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m                     \u001b[0mepisode_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                     \u001b[0miter_no\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes=50\n",
    "max_steps=50000\n",
    "save_every_n_iter=50\n",
    "log_every_n_iter=50\n",
    "initialize=False\n",
    "save_dir=\"deep_q_saves\"\n",
    "max_experience_buffer_len=10000\n",
    "initial_epsilon=0.5#1\n",
    "final_epsilon=0.0001\n",
    "epsilon_dec=0.00001\n",
    "train_t=1000\n",
    "discount_rate=0.9\n",
    "batch_size=120\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "\n",
    "model=\"\"\n",
    "with tf.Session() as sess:\n",
    "    params['input_shape']\n",
    "    if(not initialize):\n",
    "        model=Q_Network(max_experience_buffer_len,params,restore_params=True,pickle_file_path=\"deep_q_saves/q_network_with_frames/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "        model.restore_model(sess,save_dir)\n",
    "        model.params.learning_rate=0.001\n",
    "    else:\n",
    "        model=Q_Network(max_experience_buffer_len,params,restore_params=False,pickle_file_path=\"deep_q_saves/q_network_with_frames/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "    \n",
    "    model.train(sess=sess,episodes=n_episodes,steps=max_steps,initial_epsilon=initial_epsilon,final_epsilon=final_epsilon,epsilon_dec=epsilon_dec,train_t=train_t,discount_rate=discount_rate,batch_size=batch_size,env=env,save_dir=save_dir,save_every_n_iter=save_every_n_iter,log_every_n_iter=log_every_n_iter,initialize=initialize,set_logging=True)\n",
    "#     model.test(initialize=True,env=env)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a=np.arange(6).reshape(3,2)\n",
    "# # b=np.empty(6,type(a))\n",
    "# # print (b)\n",
    "# # b[1]=a\n",
    "# # print (b.shape)\n",
    "# # print(type(b))\n",
    "# # np.isnan(np.array(b))\n",
    "# a=np.array([[1,2],[3,4]])\n",
    "# # a=np.array(a)\n",
    "# print (1,*a.shape)\n",
    "# print (a.dtype)\n",
    "# a=np.array([6])\n",
    "# print (a)\n",
    "# print (np.squeeze(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring path:D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\q_network_with_frames\n",
      "INFO:tensorflow:Restoring parameters from D:\\dino_game_current\\AI_Plays_Dino_Game\\deep_q_saves\\q_network_with_frames\\model_weights.ckpt-100050\n",
      "Initializing.....\n",
      "\n",
      "Resetting Environment...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_episodes=50\n",
    "max_steps=50000\n",
    "save_every_n_iter=20\n",
    "log_every_n_iter=50\n",
    "initialize=False\n",
    "save_dir=\"deep_q_saves\"\n",
    "max_experience_buffer_len=10000\n",
    "initial_epsilon=1\n",
    "final_epsilon=0.0001\n",
    "epsilon_dec=0.00001\n",
    "train_t=1000\n",
    "discount_rate=0.9\n",
    "batch_size=120\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "\n",
    "model=\"\"\n",
    "with tf.Session() as sess:\n",
    "    params['input_shape']\n",
    "    if(not initialize):\n",
    "        model=Q_Network(max_experience_buffer_len,params,restore_params=True,pickle_file_path=\"deep_q_saves/q_network_with_frames/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "        model.restore_model(sess,save_dir)\n",
    "        model.params.learning_rate=0.0005\n",
    "    else:\n",
    "        model=Q_Network(max_experience_buffer_len,params,restore_params=False,pickle_file_path=\"deep_q_saves/q_network_with_frames/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "    \n",
    "#     model.train(sess=sess,episodes=n_episodes,steps=max_steps,initial_epsilon=initial_epsilon,final_epsilon=final_epsilon,epsilon_dec=epsilon_dec,train_t=train_t,discount_rate=discount_rate,batch_size=batch_size,env=env,save_dir=save_dir,save_every_n_iter=save_every_n_iter,log_every_n_iter=log_every_n_iter,initialize=initialize,set_logging=True)\n",
    "    model.test(initialize=True,env=env,sleep_time=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
