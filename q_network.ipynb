{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armughan.Shahid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import CNN_Model,Params\n",
    "from Env import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_replay_buffer:\n",
    "#     def __init__(self,size,dtypes):\n",
    "#         self.column_names=['state','action','next_state','reward','done']\n",
    "#         self.buffer={self.column_names[i]:np.empty(size,dtype=dtypes[i]) for i in np.arange(len(self.column_names)) }\n",
    "#         self.num_items=0\n",
    "#         self.capacity=size\n",
    "    def __init__(self,buffer_len,sample):\n",
    "        self.column_names=['state','action','next_state','reward','done']\n",
    "        self.buffer={col_name:np.empty(shape=[buffer_len,*np.array(item).shape],dtype=np.array(item).dtype) for col_name,item in zip(self.column_names,sample) }\n",
    "        self.num_items=0\n",
    "        self.capacity=buffer_len\n",
    "    def add_experience(self,state,action,next_state,reward,done):\n",
    "        ind=self.num_items\n",
    "        if self.num_items<self.capacity:\n",
    "            self.num_items+=1\n",
    "        else:\n",
    "            ind=np.random.randint(low=0,high=self.capacity,size=1,dtype=np.int32)\n",
    "            \n",
    "        self.buffer['state'][ind]=state\n",
    "        self.buffer['action'][ind]=action\n",
    "        self.buffer['next_state'][ind]=next_state\n",
    "        self.buffer['reward'][ind]=reward\n",
    "        self.buffer['done'][ind]=done\n",
    "           \n",
    "    \n",
    "    def get_batch(self,batch_size):\n",
    "        inds=np.random.randint(low=0,high=self.num_items,size=batch_size,dtype=np.int32)\n",
    "        return self.buffer['state'][inds],self.buffer['action'][inds],self.buffer['next_state'][inds],self.buffer['reward'][inds],self.buffer['done'][inds]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(CNN_Model):\n",
    "    def __init__(self,max_experience_buffer_len=120,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        CNN_Model.__init__(self,param_dict,restore_params,pickle_file_path)\n",
    "        self.max_experience_buffer_len=max_experience_buffer_len\n",
    "        \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(labels=targets,logits=logits)\n",
    "        return entropies\n",
    "        \n",
    "    def Build_model(self):\n",
    "        self.build_model_till_logits()\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            #logits are q values]\n",
    "            self.max_q_value_actions=tf.squeeze(tf.argmax(self.logits,axis=1)) #value which has the highest q value\n",
    "            self.max_q_values=tf.reduce_max(self.logits,axis=1)\n",
    "            \n",
    "            #placeholder for action at current timestep\n",
    "            self.actions=self.form_placeholder((None),tf.int32)\n",
    "            one_hot_actions=tf.one_hot(indices=self.actions,depth=self.params.num_outputs)\n",
    "            q_vals=tf.reduce_sum(self.logits*one_hot_actions,axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #placeholder for max next state q values,rewards and discount rate\n",
    "            self.max_q_values_next_state=self.form_placeholder((None),tf.float32)\n",
    "            self.rewards=self.form_placeholder((None),tf.float32)\n",
    "            self.notended=self.form_placeholder((None),tf.float32)\n",
    "            self.discount_rate=self.form_placeholder([],tf.float32)\n",
    "            \n",
    "            self.loss=tf.reduce_mean(tf.square((self.rewards+(self.discount_rate*(self.max_q_values_next_state*self.notended)))-q_vals))\n",
    "#             self.qvalues_next=self.max_q_values_next_state*self.notended\n",
    "#             self.discounted_qvalues_next=self.discount_rate*self.qvalues_next\n",
    "#             self.targets=self.rewards+self.discounted_qvalues_next\n",
    "#             self.diff=self.targets-q_vals\n",
    "#             self.squared_diff=tf.square(self.diff)\n",
    "#             self.loss=tf.reduce_mean(self.squared_diff)\n",
    "        \n",
    "            #computing gradients \n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.grads_and_vars=optimizer.compute_gradients(loss=self.loss,var_list=self.model_trainable_variables)\n",
    "            \n",
    "            self.train_op=optimizer.apply_gradients(grads_and_vars=self.grads_and_vars,global_step=self.step_no)\n",
    "            self.model_variables=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.params.name_scope)\n",
    "            self.saver=tf.train.Saver(var_list=self.model_variables)\n",
    "\n",
    "            self.initializer=tf.global_variables_initializer()\n",
    "    \n",
    "    def add_to_experience_replay(self,state,action,next_state,reward,done):\n",
    "        \n",
    "        if not hasattr(self,\"experience_replay_buffer\"):\n",
    "            sample=[state,action,next_state,reward,done]\n",
    "            self.experience_replay_buffer=experience_replay_buffer(buffer_len=self.max_experience_buffer_len,sample=sample)\n",
    "        self.experience_replay_buffer.add_experience(state,action,next_state,reward,done)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self,sess,episodes,steps,initial_epsilon,final_epsilon,epsilon_dec,train_t,discount_rate,batch_size,env,save_dir,save_every_n_iter,log_every_n_iter,initialize=False,set_logging=True):\n",
    "        if initialize:\n",
    "            print (\"Initializing.....\\n\")\n",
    "            sess.run([self.initializer])\n",
    "        if set_logging:\n",
    "            print (\"Setting up for Logging ...\\n\")\n",
    "            log_dir,set_logging=self.create_log_directory_if_doesnt_exist(save_dir)\n",
    "        if set_logging: #creating file handlers if dir cretaed or found in above statement\n",
    "            print(\"Logging called but no code implemented\")\n",
    "#                 train_writer = tf.summary.FileWriter(os.path.join(log_dir,'train'), sess.graph)\n",
    "#                 validation_writer = tf.summary.FileWriter(os.path.join(log_dir ,'validation'))\n",
    "        print (\"Retreiveing step no...\\n\")\n",
    "        [iter_no]=sess.run([self.step_no]) \n",
    "        epsilon=initial_epsilon\n",
    "        for episode in np.arange(episodes):\n",
    "            state=env.reset()\n",
    "            step=0\n",
    "            for step in np.arange(steps):\n",
    "                #choosing action \n",
    "                action=-1\n",
    "                if epsilon>final_epsilon and iter_no>train_t:\n",
    "                        epsilon-=epsilon_dec\n",
    "                    \n",
    "#                 if  iter_no<train_t or (np.random.random(1)<epsilon):\n",
    "                if  (np.random.random(1)<epsilon):\n",
    "                    action=np.random.randint(low=0,high=self.params.num_outputs,size=1,dtype=np.int32)\n",
    "                else:\n",
    "                    feed_dict={self.X:np.expand_dims(state,axis=0),self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    [action]=sess.run([self.max_q_value_actions],feed_dict=feed_dict)\n",
    "                action=np.squeeze(action)\n",
    "#                 print(action)\n",
    "                next_state,reward,done,info=env.step(action)\n",
    "                \n",
    "                self.add_to_experience_replay(state,action,next_state,reward,done)\n",
    "                \n",
    "                episode_has_finished=done\n",
    "\n",
    "                state=next_state\n",
    "\n",
    "                if self.experience_replay_buffer.num_items>train_t: #perform training if there are enough experiences\n",
    "#                     print(\"buffer filled\")\n",
    "                    \n",
    "                    \n",
    "                    #performing training step\n",
    "                    states,actions,next_states,rewards,dones=self.experience_replay_buffer.get_batch(batch_size=batch_size)\n",
    "                    \n",
    "\n",
    "                    #finding vals of next states\n",
    "#                     print (next_states.shape)\n",
    "                    feed_dict={self.X:next_states,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    [max_q_vals_next_state]=sess.run([self.max_q_values],feed_dict=feed_dict)\n",
    "#                     max_q_vals_next_state[nan_inds]=0\n",
    "\n",
    "                    #calculating loss and running train op\n",
    "#                     print(\"shapes\\n\")\n",
    "#                     print(states.shape)\n",
    "#                     print(actions.shape)\n",
    "#                     print(max_q_vals_next_state.shape)\n",
    "#                     print(rewards.shape)\n",
    "#                     print(dones.shape)\n",
    "#                     print(discount_rate)\n",
    "                    \n",
    "#                     print(\"li\\n\")\n",
    "#                     print(states)\n",
    "#                     print(actions)\n",
    "#                     print(max_q_vals_next_state)\n",
    "#                     print(rewards)\n",
    "#                     print(dones)\n",
    "#                     print(discount_rate)\n",
    "#                     self.qvalues_next=self.max_q_values_next_state*self.notended\n",
    "#             self.discounted_qvalues_next=self.discount_rate*qvalues_next\n",
    "#             self.targets=self.rewards+discounted_qvalues_next\n",
    "#             self.diff=targets-q_vals\n",
    "#             self.squared_diff=tf.square(diff)\n",
    "#             self.loss=tf.reduce_mean(squared_diff)\n",
    "                    feed_dict={self.X:states,self.actions:actions,self.max_q_values_next_state:max_q_vals_next_state,self.rewards:rewards,self.notended:((np.logical_not(dones)).astype(np.int32)),self.discount_rate:discount_rate,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    loss,_=sess.run([self.loss,self.train_op],feed_dict=feed_dict)\n",
    "\n",
    "                    iter_no+=1\n",
    "                    if (iter_no)%save_every_n_iter==0:\n",
    "                        print(\"^^^^ saving model ^^^^ \\n\")\n",
    "                        self.save_model(sess,save_dir,self.step_no)\n",
    "                    if (iter_no)%log_every_n_iter==0:\n",
    "                        print (\"Trainaing Step:\\t Iteration no={} Game Step ={} loss={} \".format(iter_no,step,loss))\n",
    "                if episode_has_finished:\n",
    "                    break\n",
    "\n",
    "            print (\"=======>Episode Length={} for Episode Number ={} <=======\\n\".format(step,episode))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting Environment...\n",
      "\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\armughan.shahid\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env=Env('SpaceInvaders-v0',convert_to_grayscale=True,crop=True,valid_Y=[20,-10],valid_X=[10,-10],resize=True,resize_Y=90,resize_X=70,normalize=True,num_of_frames_per_stack=4)\n",
    "params={\n",
    "    'input_shape':[None, *env.image_shape],\n",
    "    'num_outputs':env.action_space,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':4,'num_filters':32,'padding':'valid'},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':3,'kernel_strides':2,'num_filters':64,'padding':'valid'},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':512},\n",
    "#         {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'}\n",
    "#         {'layer_type':'dropout_layer','dropout_probability':0.2},\n",
    "#         {'layer_type':'fc_layer','num_hidden_units':50},\n",
    "# #         {'layer_type':'batch_normalization_layer'},\n",
    "#         {'layer_type':'activation_layer'}\n",
    "#         {'layer_type':'dropout_layer','dropout_probability':0.2}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.elu,\n",
    "#     'loss_fn':tf.nn.sparse_softmax_cross_entropy_with_logits, #carefull\n",
    "    'learning_rate':0.00025,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'q_network_with_frames'\n",
    "}\n",
    "print (params['num_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing.....\n",
      "\n",
      "Setting up for Logging ...\n",
      "\n",
      "Logging called but no code implemented\n",
      "Retreiveing step no...\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "Trainaing Step:\t Iteration no=5.0 Game Step =14 loss=0.9936569929122925 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=10.0 Game Step =19 loss=0.10954207181930542 \n",
      "Trainaing Step:\t Iteration no=15.0 Game Step =24 loss=0.8320862054824829 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=20.0 Game Step =29 loss=1.0945658683776855 \n",
      "Trainaing Step:\t Iteration no=25.0 Game Step =34 loss=0.5954047441482544 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=30.0 Game Step =39 loss=0.47338399291038513 \n",
      "Trainaing Step:\t Iteration no=35.0 Game Step =44 loss=2.4666380882263184 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=40.0 Game Step =49 loss=0.28419381380081177 \n",
      "=======>Episode Length=49 for Episode Number =0 <=======\n",
      "\n",
      "Resetting Environment...\n",
      "\n",
      "Trainaing Step:\t Iteration no=45.0 Game Step =4 loss=0.5968326926231384 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=50.0 Game Step =9 loss=0.3570338487625122 \n",
      "Trainaing Step:\t Iteration no=55.0 Game Step =14 loss=1.6375131607055664 \n",
      "^^^^ saving model ^^^^ \n",
      "\n",
      "Trainaing Step:\t Iteration no=60.0 Game Step =19 loss=0.19548891484737396 \n",
      "Trainaing Step:\t Iteration no=65.0 Game Step =24 loss=0.37212246656417847 \n",
      "^^^^ saving model ^^^^ \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d8b3ffde3138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_epsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_epsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon_dec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon_dec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset_logging\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-64f7b5b57de9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sess, episodes, steps, initial_epsilon, final_epsilon, epsilon_dec, train_t, discount_rate, batch_size, env, save_dir, save_every_n_iter, log_every_n_iter, initialize, set_logging)\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0msave_every_n_iter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"^^^^ saving model ^^^^ \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mlog_every_n_iter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Trainaing Step:\\t Iteration no={} Game Step ={} loss={} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_no\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\dino_game_current\\AI_Plays_Dino_Game\\model.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(self, sess, savedir, step)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_before\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m#saving model weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"model_weights.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#writes meta graph for the first time save_model is called\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrestore_params_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpickle_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[0;32m   1651\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes=50\n",
    "max_steps=50000\n",
    "save_every_n_iter=20\n",
    "log_every_n_iter=10\n",
    "initialize=True\n",
    "save_dir=\"deep_q_saves\"\n",
    "max_experience_buffer_len=1000000\n",
    "initial_epsilon=1\n",
    "final_epsilon=0.0001\n",
    "epsilon_dec=0.00001\n",
    "train_t=1000\n",
    "discount_rate=0.9\n",
    "batch_size=120\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "\n",
    "model=\"\"\n",
    "with tf.Session() as sess:\n",
    "    params['input_shape']\n",
    "    if(not initialize):\n",
    "        model=Q_Network(max_experience_buffer_len,params,restore_params=True,pickle_file_path=\"deep_q_saves/q_network_with_frames/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "        model.restore_model(sess,save_dir)\n",
    "        model.params.learning_rate=0.001\n",
    "    else:\n",
    "        model=Q_Network(max_experience_buffer_len,params,restore_params=False,pickle_file_path=\"deep_q_saves/q_network_with_frames/model_object.pkl\")\n",
    "        model.Build_model()\n",
    "    \n",
    "    model.train(sess=sess,episodes=n_episodes,steps=max_steps,initial_epsilon=initial_epsilon,final_epsilon=final_epsilon,epsilon_dec=epsilon_dec,train_t=train_t,discount_rate=discount_rate,batch_size=batch_size,env=env,save_dir=save_dir,save_every_n_iter=save_every_n_iter,log_every_n_iter=log_every_n_iter,initialize=initialize,set_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# # a=np.arange(6).reshape(3,2)\n",
    "# # b=np.empty(6,type(a))\n",
    "# # print (b)\n",
    "# # b[1]=a\n",
    "# # print (b.shape)\n",
    "# # print(type(b))\n",
    "# # np.isnan(np.array(b))\n",
    "# a=np.array([[1,2],[3,4]])\n",
    "# # a=np.array(a)\n",
    "# print (1,*a.shape)\n",
    "# print (a.dtype)\n",
    "# a=np.array([6])\n",
    "# print (a)\n",
    "# print (np.squeeze(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (env.image_shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
