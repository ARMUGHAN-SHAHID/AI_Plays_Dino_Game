{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armughan.Shahid\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import CNN_Model,Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_replay_buffer:\n",
    "    def __init__(self,size,dtype):\n",
    "        self.buffer=np.empty(size,dtype=dtype)\n",
    "        self.num_items=0\n",
    "        self.capacity=size\n",
    "    def add_experience(self,experience):\n",
    "        \n",
    "        if self.num_items<self.capacity:\n",
    "            self.buffer[self.num_items]=experience\n",
    "            self.num_items+=1\n",
    "        else:\n",
    "            ind=np.random.randint(low=0,high=self.capacity,size=1,dtype=np.int32)\n",
    "            self.buffer[ind]=experience\n",
    "    \n",
    "    def get_batch(self,batch_size):\n",
    "        inds=np.random.randint(low=0,high=self.capacity,size=batch_size,dtype=np.int32)\n",
    "        experiences=self.experience_replay_buffer[inds]\n",
    "        df=pd.DataFrame(experiences, columns=['states', 'actions', 'next_states','rewards'])\n",
    "        return np.array(df.states),np.array(df.actions),np.array(df.next_states),np.array(df.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(CNN_Model):\n",
    "    def __init__(self,max_experience_buffer_len=120,param_dict={},restore_params=False,pickle_file_path=\"\"):\n",
    "        CNN_Model.__init__(self,param_dict,restore_params,pickle_file_path)\n",
    "        self.max_experience_buffer_len=max_experience_buffer_len\n",
    "        \n",
    "    def form_loss(self,logits,targets):\n",
    "        entropies=self.params.loss_fn(labels=targets,logits=logits)\n",
    "        return entropies\n",
    "        \n",
    "    def Build_model(self):\n",
    "        self.build_model_till_logits()\n",
    "        with tf.variable_scope(self.params.name_scope):\n",
    "            #logits are q values]\n",
    "#             self.max_q_value_actions=tf.argmax(self.logits,axis=1) #value which has the highest q value\n",
    "#             self.max_q_value_actions_one_hot=tf.one_hot(self.max_q_value_actions,depth=self.params.num_outputs)\n",
    "#             self.max_q_values=self.logits*self.max_q_value_actions_one_hot\n",
    "            self.max_q_value_actions=tf.squeeze(tf.argmax(self.logits,axis=1)) #value which has the highest q value\n",
    "            self.max_q_values=tf.reduce_max(self.logits,axis=1)\n",
    "            \n",
    "#             print (self.max_q_values.shape)\n",
    "            #placeholder for action at current timestep\n",
    "#             self.one_hot_actions=self.form_placeholder((None,self.params.num_outputs),tf.float32) #actions are not one hot\n",
    "#             self.one_hot_actions=tf.one_hot(indices=self.actions,depth=self.params.num_outputs)\n",
    "#             q_vals=self.logits*self.one_hot_actions\n",
    "            self.actions=self.form_placeholder((None),tf.int32)\n",
    "            one_hot_actions=tf.one_hot(indices=self.actions,depth=self.params.num_outputs)\n",
    "            q_vals=tf.reduce_sum(self.logits*one_hot_actions,axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #placeholder for max next state q values,rewards and discount rate\n",
    "#             self.max_q_values_next_state=self.form_placeholder((None,self.params.num_outputs),tf.float32)\n",
    "            self.max_q_values_next_state=self.form_placeholder((None),tf.float32)\n",
    "            self.rewards=self.form_placeholder((None),tf.float32)\n",
    "            self.discount_rate=self.form_placeholder([],tf.float32)\n",
    "            \n",
    "            self.loss=tf.reduce_mean(tf.square(q_vals-(self.rewards+(self.discount_rate*self.max_q_values_next_state))))\n",
    "            #computing gradients \n",
    "            optimizer=self.params.optimizer_fn(learning_rate=self.lr_placeholder)\n",
    "            self.grads_and_vars=optimizer.compute_gradients(loss=self.loss,var_list=self.model_trainable_variables)\n",
    "            \n",
    "            self.train_op=optimizer.apply_gradients(grads_and_vars=self.grads_and_vars,global_step=self.step_no)\n",
    "            \n",
    "            self.initializer=tf.global_variables_initializer()\n",
    "        def add_to_experience_replay(self,state,action,next_state,reward):\n",
    "            experience=tuple(state,action,next_state,reward)\n",
    "            if not hasattr(\"experience_replay_buffer\"):\n",
    "                self.experience_replay_buffer=experience_replay_buffer(size=self.max_experience_buffer_len,dtype=type(experience))\n",
    "            self.experience_replay_buffer.add_experience(experience)\n",
    "        \n",
    "       \n",
    "     \n",
    "        def train(self,sess,episodes,steps,epsilon,batch_size,env,save_dir,save_every_n_iter,log_every_n_iter,initialize=False,set_logging=True):\n",
    "            if initialize:\n",
    "                sess.run([self.initializer])\n",
    "            if set_logging:\n",
    "                log_dir,set_logging=self.create_log_directory_if_doesnt_exist(save_dir)\n",
    "            if set_logging: #creating file handlers if dir cretaed or found in above statement\n",
    "                print(\"logging called but no code implemented\")\n",
    "#                 train_writer = tf.summary.FileWriter(os.path.join(log_dir,'train'), sess.graph)\n",
    "#                 validation_writer = tf.summary.FileWriter(os.path.join(log_dir ,'validation'))\n",
    "            [step_no]=sess.run([self.step_no]) \n",
    "            \n",
    "            for episode in np.arange(episodes):\n",
    "                state=env.reset()\n",
    "                for step in np.arange(steps):\n",
    "                    #choosing action \n",
    "                    action=-1 \n",
    "                    if (np.random.random(1)<epsilon):\n",
    "                        action=np.random.randint(low=0,high=self.params.num_outputs,size=1,dtype=np.int32)\n",
    "                    else:\n",
    "                        feed_dict={self.X:np.expand_dims(state,axis=0),self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                        action=sess.run([self.max_q_value_actions],feed_dict=feed_dict)\n",
    "                        action=action[0]\n",
    "                        \n",
    "                    next_state,reward,done,info=env.step(action)\n",
    "                    \n",
    "                    if done:\n",
    "                        next_state=np.nan\n",
    "                        self.add_to_experience_replay(state,action,next_state,reward)\n",
    "                        break\n",
    "                    else:\n",
    "                        self.add_to_experience_replay(state,action,next_state,reward)\n",
    "                        state=next_state\n",
    "                        \n",
    "                    #performing training step\n",
    "                    states,actions,next_states,rewards=self.experience_replay_buffer.get_batch(batch_size=batch_size)\n",
    "                    \n",
    "                    #finding where the next states are nans (episode ended)\n",
    "                    nan_inds=np.isnan(next_states)\n",
    "#                     filetered_next_states=next_states[np.logical_not(nan_inds)]\n",
    "                    next_states[nan_inds]=np.zeros(shape=self.params.input_shape[1:],dtype=np.float32)\n",
    "                    \n",
    "                    #finding vals of next states\n",
    "                    feed_dict={self.X:next_states,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    max_q_vals_next_state=sess.run([self.max_q_values],feed_dict=feed_dict)\n",
    "                    max_q_vals_next_state[nan_inds]=0\n",
    "                    \n",
    "                    #calculating loss and running train op\n",
    "                    feed_dict={self.X:states,self.actions:actions,self.max_q_values_next_state:max_q_values_next_state,self.rewards:rewards,self.lr_placeholder:self.params.learning_rate,self.training_mode:True}\n",
    "                    loss,_=sess.run([self.loss,self.train_op],feed_dict=feed_dict)\n",
    "#             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'input_shape':[None, 35, 190, 1],\n",
    "    'num_outputs':3,\n",
    "    \n",
    "    'layer_hierarchy':[\n",
    "        {'layer_type':'conv_layer','kernel_size':8,'kernel_strides':1,'num_filters':16,'padding':'valid'},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'conv_layer','kernel_size':4,'kernel_strides':1,'num_filters':32,'padding':'valid'},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'flattening_layer'},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':256},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'dropout_layer','dropout_probability':0.5},\n",
    "        {'layer_type':'fc_layer','num_hidden_units':100},\n",
    "        {'layer_type':'batch_normalization_layer'},\n",
    "        {'layer_type':'activation_layer'},\n",
    "        {'layer_type':'dropout_layer','dropout_probability':0.5}\n",
    "        \n",
    "    ],\n",
    "    'initializer_fn':tf.contrib.layers.variance_scaling_initializer,\n",
    "    'activation_fn':tf.nn.relu,\n",
    "    'loss_fn':tf.nn.sparse_softmax_cross_entropy_with_logits, #carefull\n",
    "    'learning_rate':0.001,\n",
    "    'optimizer_fn':tf.train.AdamOptimizer,\n",
    "    'logdir':'/tf_logs_rnn/run/',\n",
    "    'name_scope':'neural_network_bn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self,max_experience_buffer_len=120,param_dict={},restore_params=False,pickle_file_path=\"\"\n",
    "tf.reset_default_graph()\n",
    "model=Q_Network(120,params)\n",
    "model.Build_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
